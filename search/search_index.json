{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PD Penambangan Data, Data Mining , Soli Deo Gloria of data NAMA: Wildan Mubarok NIM: 180411100106 PEMBIMBING: @mulaab REFERENSI: https://mulaab.github.io/datamining IPYNB SOURCE: https://github.com/willnode/PD/tree/master/src Kamus Data Mining Core Concept Data adalah tabel, multi-n-matriks Kolom adalah fitur Baris adalah sampel \"Eureka\" data mining adalah mengubah data sampah menjadi data emas Jenis Data Numerik Skala Interval Skala Ratio Kategorikal Biner Nominal Prosedur Business Understanding Prepocessing Modeling Evaluation Deployment Toolkit Built-ins: statistics itertools Pandas NumPy SciPy Matplotlib Scikit-Learn","title":"PD"},{"location":"#pd","text":"Penambangan Data, Data Mining , Soli Deo Gloria of data NAMA: Wildan Mubarok NIM: 180411100106 PEMBIMBING: @mulaab REFERENSI: https://mulaab.github.io/datamining IPYNB SOURCE: https://github.com/willnode/PD/tree/master/src","title":"PD"},{"location":"#kamus-data-mining","text":"Core Concept Data adalah tabel, multi-n-matriks Kolom adalah fitur Baris adalah sampel \"Eureka\" data mining adalah mengubah data sampah menjadi data emas Jenis Data Numerik Skala Interval Skala Ratio Kategorikal Biner Nominal Prosedur Business Understanding Prepocessing Modeling Evaluation Deployment Toolkit Built-ins: statistics itertools Pandas NumPy SciPy Matplotlib Scikit-Learn","title":"Kamus Data Mining"},{"location":"PD_01_Statistik_Deskriptif/","text":"Statistik Deskriptif Setiap kelompok data numerik mempunyai properti tersendiri yang menjelaskan secara unik data tersebut: Sebelum mulai, mari kita mengambil beberapa sampel data import pandas as pd import statistics , itertools from IPython.display import HTML , display from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = pd . read_csv ( 'leaf.csv' , nrows = 9 , usecols = [ 'Aspect Ratio' ]) # Ambil Sampel data = [ round ( x [ 0 ], 1 ) for x in df . values ] # Bulat-bulat df = pd . DataFrame ( data , columns = [ 'sample' ]); # Data Frame dari Sample dc = df [ 'sample' ] # Data set kolom sample dc 0 1 . 5 1 1 . 5 2 1 . 6 3 1 . 5 4 1 . 8 5 1 . 5 6 1 . 8 7 1 . 6 8 1 . 8 Name : sample , dtype : float64 Data ini dapat kita deskripsikan menggunakan properti-properti berikut. Mean Mean adalah rata-rata dari suatu dataset. Diperoleh dari sum dataset lalu dibagi dengan jumlah elemen dataset. Biasa disimbolkan sebagai \\mu \\mu \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} print ( \"Rata-rata\" , dc . values , \"=\" , dc . mean ()) Rata - rata [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] = 1 . 6222222222222222 Mean dalam built-in python: print ( \"Rata-rata\" , data , \"=\" , statistics . mean ( data )) Rata - rata [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 1 . 6222222222222222 Median Median merupakan titik data yang paling baik apabila dataset telah diurutkan. Dalam data numerik non interval, data ke-(n-1)/2 adalah median jika n ganjil atau rata-rata dari data ke-(n/2) dan data ke-(n/2+1) jika n genap. print ( \"Median\" , dc . values , \"=\" , dc . median ()) Median [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] = 1 . 6 Median dalam built-in python: print ( \"Median\" , data , \"=\" , statistics . median ( data )) Median [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 1 . 6 Pembuktian dengan menyortir + eliminasi data: sorteddata = data [:]; sorteddata . sort (); print ( sorteddata ) while ( len ( sorteddata ) > 1 ): sorteddata = sorteddata [ 1 : - 1 ] print ( sorteddata ) [1.5, 1.5, 1.5, 1.5, 1.6, 1.6, 1.8, 1.8, 1.8] [1.5, 1.5, 1.5, 1.6, 1.6, 1.8, 1.8] [1.5, 1.5, 1.6, 1.6, 1.8] [1.5, 1.6, 1.6] [1.6] Mode Mode merupakan statistik untuk angka mana yang paling banyak frekuensinya dalam dataset. Mode bisa dalam bentuk diskrit atau kelompok. Mode (diskrik) dalam built-in python: print ( \"Median\" , data , \"=\" , statistics . mode ( data )) Median [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 1 . 5 scipy mempunyai tool untuk mendeteksi mode secara lebih detail jika ada >1 value dengan frekuensi yang sama from scipy import stats from numpy import transpose modedata = stats . mode ( dc ) table ( pd . DataFrame ( transpose ([ modedata . mode , modedata . count ]), columns = [ \"Mode\" , \"Count\" ])) Mode Count 1.5 4 Gunakan seaborn untuk melihat frekuensi secara grafikal: from seaborn import distplot ax = distplot ( data ) Range Range dalam suatu dataset ialah angka tertinggi dan angka terendah dalam dataset. print ( \"Range\" , dc . values , \", Max:\" , dc . max (), \"Min:\" , dc . min ()) Range [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] , Max : 1 . 8 Min : 1 . 5 Range dalam built-in python: print ( \"Range\" , data , \", Max:\" , max ( data ), \"Min:\" , min ( data )) Range [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] , Max : 1 . 8 Min : 1 . 5 Quantile Quantil adalah jarak data yang memisahkan data sekin persen dari yang terkecil hingga tertinggi. Quantil dipisah menjadi: + Q1 sebagai Quantil bawah (25%) + Q2 sebagai Quantil tengah (50%) + Q3 sebagai Quantil atas (75%) print ( \"Quantil\" , dc . values , \"Q1:\" , dc . quantile ( 0.25 ), \"Q2:\" , dc . quantile ( 0.5 ), \"Q3:\" , dc . quantile ( 0.75 )) Quantil [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] Q1 : 1 . 5 Q2 : 1 . 6 Q3 : 1 . 8 Quantil bisa dihitung menggunakan numpy : from numpy import quantile print ( \"Quantil\" , data , \"Q1:\" , quantile ( data , 0.25 ), \"Q2:\" , dc . quantile ( 0.5 ), \"Q3:\" , dc . quantile ( 0.75 )) Quantil [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] Q1 : 1 . 5 Q2 : 1 . 6 Q3 : 1 . 8 Variance Properti tentang seberapa jauh nilai dari rata-rata (alias variasi). Biasa disimbolkan \\sigma^2 \\sigma^2 print ( \"Variansi\" , dc . values , \"=\" , dc . var ()) Variansi [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] = 0 . 01944444444444445 print ( \"Variansi\" , data , \"=\" , statistics . variance ( data )) Variansi [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 0 . 01944444444444445 Standar Deviasi Standar nilai tentang seberapa jauh data dari mean. Rumus: \\sqrt{\\frac{\\sum_{i=1}^n\\left(x_i-\\overline{x}\\right)^2}{n-1}} \\sqrt{\\frac{\\sum_{i=1}^n\\left(x_i-\\overline{x}\\right)^2}{n-1}} print ( \"Standar Deviasi\" , dc . values , \"=\" , dc . std ()) Standar Deviasi [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] = 0 . 1394433377556793 print ( \"Variansi\" , data , \"=\" , statistics . stdev ( data )) Variansi [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 0 . 1394433377556793 Summary Deskripsi Data set dalam leaf.csv : adf = pd . read_csv ( 'leaf.csv' ) adata = [[ x , '{:.2f}' . format ( adf [ x ] . mean ()), '{:.2f}' . format ( adf [ x ] . median ()), '{:.2f}' . format ( adf [ x ] . min ()), '{:.2f}' . format ( adf [ x ] . max ()), '{:.2f}' . format ( adf [ x ] . skew ()), '{:.2f}' . format ( adf [ x ] . var ()), '{:.2f}' . format ( adf [ x ] . std ())] for x in adf . columns [ 1 :]] table ( pd . DataFrame ( adata , columns = [ 'Nama Kolom' , 'Mean' , 'Median' , 'Min' , 'Max' , 'Skew' , 'Var' , 'Std' ])) Nama Kolom Mean Median Min Max Skew Var Std Specimen Number 6.28 6 1 16 0.2 11.99 3.46 Eccentricity 0.72 0.76 0.12 1 -0.56 0.04 0.21 Aspect Ratio 2.44 1.57 1.01 19.04 3.33 6.76 2.6 Elongation 0.51 0.5 0.11 0.95 0.34 0.04 0.2 Solidity 0.9 0.95 0.49 0.99 -2.06 0.01 0.11 Stochastic Convexity 0.94 0.99 0.4 1 -2.63 0.01 0.12 Isoperimetric Factor 0.53 0.58 0.08 0.86 -0.48 0.05 0.22 Maximal Indentation Depth 0.04 0.02 0 0.2 1.71 0 0.04 Lobedness 0.52 0.1 0 7.21 3.12 1.08 1.04 Average Intensity 0.05 0.04 0.01 0.19 0.94 0 0.04 Average Contrast 0.12 0.12 0.03 0.28 0.47 0 0.05 Smoothness 0.02 0.01 0 0.07 1.21 0 0.01 Third moment 0.01 0 0 0.03 1.78 0 0.01 Uniformity 0 0 0 0 2.13 0 0 Entropy 1.16 1.08 0.17 2.71 0.49 0.34 0.58","title":"Statistik Deskriptif"},{"location":"PD_01_Statistik_Deskriptif/#statistik-deskriptif","text":"Setiap kelompok data numerik mempunyai properti tersendiri yang menjelaskan secara unik data tersebut: Sebelum mulai, mari kita mengambil beberapa sampel data import pandas as pd import statistics , itertools from IPython.display import HTML , display from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = pd . read_csv ( 'leaf.csv' , nrows = 9 , usecols = [ 'Aspect Ratio' ]) # Ambil Sampel data = [ round ( x [ 0 ], 1 ) for x in df . values ] # Bulat-bulat df = pd . DataFrame ( data , columns = [ 'sample' ]); # Data Frame dari Sample dc = df [ 'sample' ] # Data set kolom sample dc 0 1 . 5 1 1 . 5 2 1 . 6 3 1 . 5 4 1 . 8 5 1 . 5 6 1 . 8 7 1 . 6 8 1 . 8 Name : sample , dtype : float64 Data ini dapat kita deskripsikan menggunakan properti-properti berikut.","title":"Statistik Deskriptif"},{"location":"PD_01_Statistik_Deskriptif/#mean","text":"Mean adalah rata-rata dari suatu dataset. Diperoleh dari sum dataset lalu dibagi dengan jumlah elemen dataset. Biasa disimbolkan sebagai \\mu \\mu \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} print ( \"Rata-rata\" , dc . values , \"=\" , dc . mean ()) Rata - rata [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] = 1 . 6222222222222222 Mean dalam built-in python: print ( \"Rata-rata\" , data , \"=\" , statistics . mean ( data )) Rata - rata [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 1 . 6222222222222222","title":"Mean"},{"location":"PD_01_Statistik_Deskriptif/#median","text":"Median merupakan titik data yang paling baik apabila dataset telah diurutkan. Dalam data numerik non interval, data ke-(n-1)/2 adalah median jika n ganjil atau rata-rata dari data ke-(n/2) dan data ke-(n/2+1) jika n genap. print ( \"Median\" , dc . values , \"=\" , dc . median ()) Median [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] = 1 . 6 Median dalam built-in python: print ( \"Median\" , data , \"=\" , statistics . median ( data )) Median [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 1 . 6 Pembuktian dengan menyortir + eliminasi data: sorteddata = data [:]; sorteddata . sort (); print ( sorteddata ) while ( len ( sorteddata ) > 1 ): sorteddata = sorteddata [ 1 : - 1 ] print ( sorteddata ) [1.5, 1.5, 1.5, 1.5, 1.6, 1.6, 1.8, 1.8, 1.8] [1.5, 1.5, 1.5, 1.6, 1.6, 1.8, 1.8] [1.5, 1.5, 1.6, 1.6, 1.8] [1.5, 1.6, 1.6] [1.6]","title":"Median"},{"location":"PD_01_Statistik_Deskriptif/#mode","text":"Mode merupakan statistik untuk angka mana yang paling banyak frekuensinya dalam dataset. Mode bisa dalam bentuk diskrit atau kelompok. Mode (diskrik) dalam built-in python: print ( \"Median\" , data , \"=\" , statistics . mode ( data )) Median [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 1 . 5 scipy mempunyai tool untuk mendeteksi mode secara lebih detail jika ada >1 value dengan frekuensi yang sama from scipy import stats from numpy import transpose modedata = stats . mode ( dc ) table ( pd . DataFrame ( transpose ([ modedata . mode , modedata . count ]), columns = [ \"Mode\" , \"Count\" ])) Mode Count 1.5 4 Gunakan seaborn untuk melihat frekuensi secara grafikal: from seaborn import distplot ax = distplot ( data )","title":"Mode"},{"location":"PD_01_Statistik_Deskriptif/#range","text":"Range dalam suatu dataset ialah angka tertinggi dan angka terendah dalam dataset. print ( \"Range\" , dc . values , \", Max:\" , dc . max (), \"Min:\" , dc . min ()) Range [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] , Max : 1 . 8 Min : 1 . 5 Range dalam built-in python: print ( \"Range\" , data , \", Max:\" , max ( data ), \"Min:\" , min ( data )) Range [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] , Max : 1 . 8 Min : 1 . 5","title":"Range"},{"location":"PD_01_Statistik_Deskriptif/#quantile","text":"Quantil adalah jarak data yang memisahkan data sekin persen dari yang terkecil hingga tertinggi. Quantil dipisah menjadi: + Q1 sebagai Quantil bawah (25%) + Q2 sebagai Quantil tengah (50%) + Q3 sebagai Quantil atas (75%) print ( \"Quantil\" , dc . values , \"Q1:\" , dc . quantile ( 0.25 ), \"Q2:\" , dc . quantile ( 0.5 ), \"Q3:\" , dc . quantile ( 0.75 )) Quantil [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] Q1 : 1 . 5 Q2 : 1 . 6 Q3 : 1 . 8 Quantil bisa dihitung menggunakan numpy : from numpy import quantile print ( \"Quantil\" , data , \"Q1:\" , quantile ( data , 0.25 ), \"Q2:\" , dc . quantile ( 0.5 ), \"Q3:\" , dc . quantile ( 0.75 )) Quantil [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] Q1 : 1 . 5 Q2 : 1 . 6 Q3 : 1 . 8","title":"Quantile"},{"location":"PD_01_Statistik_Deskriptif/#variance","text":"Properti tentang seberapa jauh nilai dari rata-rata (alias variasi). Biasa disimbolkan \\sigma^2 \\sigma^2 print ( \"Variansi\" , dc . values , \"=\" , dc . var ()) Variansi [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] = 0 . 01944444444444445 print ( \"Variansi\" , data , \"=\" , statistics . variance ( data )) Variansi [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 0 . 01944444444444445","title":"Variance"},{"location":"PD_01_Statistik_Deskriptif/#standar-deviasi","text":"Standar nilai tentang seberapa jauh data dari mean. Rumus: \\sqrt{\\frac{\\sum_{i=1}^n\\left(x_i-\\overline{x}\\right)^2}{n-1}} \\sqrt{\\frac{\\sum_{i=1}^n\\left(x_i-\\overline{x}\\right)^2}{n-1}} print ( \"Standar Deviasi\" , dc . values , \"=\" , dc . std ()) Standar Deviasi [ 1 . 5 1 . 5 1 . 6 1 . 5 1 . 8 1 . 5 1 . 8 1 . 6 1 . 8 ] = 0 . 1394433377556793 print ( \"Variansi\" , data , \"=\" , statistics . stdev ( data )) Variansi [ 1 . 5 , 1 . 5 , 1 . 6 , 1 . 5 , 1 . 8 , 1 . 5 , 1 . 8 , 1 . 6 , 1 . 8 ] = 0 . 1394433377556793","title":"Standar Deviasi"},{"location":"PD_01_Statistik_Deskriptif/#summary","text":"Deskripsi Data set dalam leaf.csv : adf = pd . read_csv ( 'leaf.csv' ) adata = [[ x , '{:.2f}' . format ( adf [ x ] . mean ()), '{:.2f}' . format ( adf [ x ] . median ()), '{:.2f}' . format ( adf [ x ] . min ()), '{:.2f}' . format ( adf [ x ] . max ()), '{:.2f}' . format ( adf [ x ] . skew ()), '{:.2f}' . format ( adf [ x ] . var ()), '{:.2f}' . format ( adf [ x ] . std ())] for x in adf . columns [ 1 :]] table ( pd . DataFrame ( adata , columns = [ 'Nama Kolom' , 'Mean' , 'Median' , 'Min' , 'Max' , 'Skew' , 'Var' , 'Std' ])) Nama Kolom Mean Median Min Max Skew Var Std Specimen Number 6.28 6 1 16 0.2 11.99 3.46 Eccentricity 0.72 0.76 0.12 1 -0.56 0.04 0.21 Aspect Ratio 2.44 1.57 1.01 19.04 3.33 6.76 2.6 Elongation 0.51 0.5 0.11 0.95 0.34 0.04 0.2 Solidity 0.9 0.95 0.49 0.99 -2.06 0.01 0.11 Stochastic Convexity 0.94 0.99 0.4 1 -2.63 0.01 0.12 Isoperimetric Factor 0.53 0.58 0.08 0.86 -0.48 0.05 0.22 Maximal Indentation Depth 0.04 0.02 0 0.2 1.71 0 0.04 Lobedness 0.52 0.1 0 7.21 3.12 1.08 1.04 Average Intensity 0.05 0.04 0.01 0.19 0.94 0 0.04 Average Contrast 0.12 0.12 0.03 0.28 0.47 0 0.05 Smoothness 0.02 0.01 0 0.07 1.21 0 0.01 Third moment 0.01 0 0 0.03 1.78 0 0.01 Uniformity 0 0 0 0 2.13 0 0 Entropy 1.16 1.08 0.17 2.71 0.49 0.34 0.58","title":"Summary"},{"location":"PD_02_Jarak_Data/","text":"Jarak Data Data dapat diketahui equivalensinya menggunakan penggurukan jarak dari dataset. Sebelum itu, mari kita tarik sampel data dan beri label. from pandas import * import itertools import scipy.spatial.distance as spad columns = [ 'Specimen Number' , 'Eccentricity' , 'Aspect Ratio' , 'Elongation' , 'Solidity' ] df = read_csv ( 'leaf.csv' , nrows = 4 , usecols = columns ) data = [[[ \"\" , \"A\" , \"B\" , \"C\" , \"D\" ][ int ( x [ 0 ])]] + [ round ( i * 2 , 2 ) for i in x [ 1 :]] for x in df . values . tolist ()] df = DataFrame ( data , columns = [ 'Class' ] + columns [ 1 :]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Class Eccentricity Aspect Ratio Elongation Solidity 0 A 1.45 2.95 0.65 1.97 1 B 1.48 3.05 0.72 1.96 2 C 1.53 3.15 0.78 1.96 3 D 1.48 2.92 0.71 1.95 Minkowski Distance Jarak Minkowski adalah jarak spatial antara dua record ( x x dan y y ) dengan m m sebagai parameter real dan n n sebagai jumlah dimensi pada entity. d_{\\operatorname{minkowski}} = \\left(\\sum_{i=1}^{n}|x_{i}-y_{i}|^{m}\\right)^{\\frac{1}{m}},m\\geq 1 d_{\\operatorname{minkowski}} = \\left(\\sum_{i=1}^{n}|x_{i}-y_{i}|^{m}\\right)^{\\frac{1}{m}},m\\geq 1 Special Case: + Jika M = 1 maka bisa disebut Manhattan (Cityblock) distance d_{\\operatorname{manhattan}} = \\sum_{i=1}^{n}|x_{i}-y_{i}| d_{\\operatorname{manhattan}} = \\sum_{i=1}^{n}|x_{i}-y_{i}| Jika M = 2 maka bisa disebut Euclidean distance. d_{\\operatorname{euclidean}} = \\sqrt{\\sum_{i=1}^{n}|x_{i}-y_{i}|^{2}} d_{\\operatorname{euclidean}} = \\sqrt{\\sum_{i=1}^{n}|x_{i}-y_{i}|^{2}} Ilustrasi menggambarkan hitungan Cityblock (garis putus-putus) dan Euclidean (garis lurus) secara visual: columns = [ 'v1-v2' , 'Manhattan distance (M=1)' , 'Euclidean distance (M=2)' , 'Minkowski distance at M=3' ] data2 = [( \"{} - {}\" . format ( a [ 0 ], b [ 0 ]), spad . cityblock ( a [ 1 :], b [ 1 :]), spad . euclidean ( a [ 1 :], b [ 1 :]), spad . minkowski ( a [ 1 :], b [ 1 :], 3 ), ) for a , b in itertools . combinations ( data , 2 )] DataFrame ( data2 , columns = columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1-v2 Manhattan distance (M=1) Euclidean distance (M=2) Minkowski distance at M=3 0 A - B 0.21 0.126095 0.111091 1 A - C 0.42 0.251794 0.220426 2 A - D 0.14 0.076158 0.065265 3 B - C 0.21 0.126886 0.110275 4 B - D 0.15 0.130767 0.130039 5 C - D 0.36 0.245764 0.232918 Average Distance columns = [ 'v1-v2' , 'Average Distance' , 'Euclidean distance (M=2)' , 'Minkowski M=3' ] data2 = [( \"{} - {}\" . format ( a [ 0 ], b [ 0 ]), spad . cityblock ( a [ 1 :], b [ 1 :]), spad . euclidean ( a [ 1 :], b [ 1 :]), spad . euclidean ( a [ 1 :], b [ 1 :]), ) for a , b in itertools . combinations ( data , 2 )] DataFrame ( data2 , columns = columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1-v2 Average Distance Euclidean distance (M=2) Minkowski M=3 0 A - B 0.21 0.126095 0.126095 1 A - C 0.42 0.251794 0.251794 2 A - D 0.14 0.076158 0.076158 3 B - C 0.21 0.126886 0.126886 4 B - D 0.15 0.130767 0.130767 5 C - D 0.36 0.245764 0.245764 Weighted Distance Chord distance Mahalanobis distance Cosine Measure Pearson correlation Summary","title":"Jarak Data"},{"location":"PD_02_Jarak_Data/#jarak-data","text":"Data dapat diketahui equivalensinya menggunakan penggurukan jarak dari dataset. Sebelum itu, mari kita tarik sampel data dan beri label. from pandas import * import itertools import scipy.spatial.distance as spad columns = [ 'Specimen Number' , 'Eccentricity' , 'Aspect Ratio' , 'Elongation' , 'Solidity' ] df = read_csv ( 'leaf.csv' , nrows = 4 , usecols = columns ) data = [[[ \"\" , \"A\" , \"B\" , \"C\" , \"D\" ][ int ( x [ 0 ])]] + [ round ( i * 2 , 2 ) for i in x [ 1 :]] for x in df . values . tolist ()] df = DataFrame ( data , columns = [ 'Class' ] + columns [ 1 :]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Class Eccentricity Aspect Ratio Elongation Solidity 0 A 1.45 2.95 0.65 1.97 1 B 1.48 3.05 0.72 1.96 2 C 1.53 3.15 0.78 1.96 3 D 1.48 2.92 0.71 1.95","title":"Jarak Data"},{"location":"PD_02_Jarak_Data/#minkowski-distance","text":"Jarak Minkowski adalah jarak spatial antara dua record ( x x dan y y ) dengan m m sebagai parameter real dan n n sebagai jumlah dimensi pada entity. d_{\\operatorname{minkowski}} = \\left(\\sum_{i=1}^{n}|x_{i}-y_{i}|^{m}\\right)^{\\frac{1}{m}},m\\geq 1 d_{\\operatorname{minkowski}} = \\left(\\sum_{i=1}^{n}|x_{i}-y_{i}|^{m}\\right)^{\\frac{1}{m}},m\\geq 1 Special Case: + Jika M = 1 maka bisa disebut Manhattan (Cityblock) distance d_{\\operatorname{manhattan}} = \\sum_{i=1}^{n}|x_{i}-y_{i}| d_{\\operatorname{manhattan}} = \\sum_{i=1}^{n}|x_{i}-y_{i}| Jika M = 2 maka bisa disebut Euclidean distance. d_{\\operatorname{euclidean}} = \\sqrt{\\sum_{i=1}^{n}|x_{i}-y_{i}|^{2}} d_{\\operatorname{euclidean}} = \\sqrt{\\sum_{i=1}^{n}|x_{i}-y_{i}|^{2}} Ilustrasi menggambarkan hitungan Cityblock (garis putus-putus) dan Euclidean (garis lurus) secara visual: columns = [ 'v1-v2' , 'Manhattan distance (M=1)' , 'Euclidean distance (M=2)' , 'Minkowski distance at M=3' ] data2 = [( \"{} - {}\" . format ( a [ 0 ], b [ 0 ]), spad . cityblock ( a [ 1 :], b [ 1 :]), spad . euclidean ( a [ 1 :], b [ 1 :]), spad . minkowski ( a [ 1 :], b [ 1 :], 3 ), ) for a , b in itertools . combinations ( data , 2 )] DataFrame ( data2 , columns = columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1-v2 Manhattan distance (M=1) Euclidean distance (M=2) Minkowski distance at M=3 0 A - B 0.21 0.126095 0.111091 1 A - C 0.42 0.251794 0.220426 2 A - D 0.14 0.076158 0.065265 3 B - C 0.21 0.126886 0.110275 4 B - D 0.15 0.130767 0.130039 5 C - D 0.36 0.245764 0.232918","title":"Minkowski Distance"},{"location":"PD_02_Jarak_Data/#average-distance","text":"columns = [ 'v1-v2' , 'Average Distance' , 'Euclidean distance (M=2)' , 'Minkowski M=3' ] data2 = [( \"{} - {}\" . format ( a [ 0 ], b [ 0 ]), spad . cityblock ( a [ 1 :], b [ 1 :]), spad . euclidean ( a [ 1 :], b [ 1 :]), spad . euclidean ( a [ 1 :], b [ 1 :]), ) for a , b in itertools . combinations ( data , 2 )] DataFrame ( data2 , columns = columns ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } v1-v2 Average Distance Euclidean distance (M=2) Minkowski M=3 0 A - B 0.21 0.126095 0.126095 1 A - C 0.42 0.251794 0.251794 2 A - D 0.14 0.076158 0.076158 3 B - C 0.21 0.126886 0.126886 4 B - D 0.15 0.130767 0.130767 5 C - D 0.36 0.245764 0.245764","title":"Average Distance"},{"location":"PD_02_Jarak_Data/#weighted-distance","text":"","title":"Weighted Distance"},{"location":"PD_02_Jarak_Data/#chord-distance","text":"","title":"Chord distance"},{"location":"PD_02_Jarak_Data/#mahalanobis-distance","text":"","title":"Mahalanobis distance"},{"location":"PD_02_Jarak_Data/#cosine-measure","text":"","title":"Cosine Measure"},{"location":"PD_02_Jarak_Data/#pearson-correlation","text":"","title":"Pearson correlation"},{"location":"PD_02_Jarak_Data/#summary","text":"","title":"Summary"},{"location":"PD_03_Seleksi_Sampel/","text":"Seleksi Sampel Mari kita buat beberapa sampel import pandas as pd import statistics , itertools from IPython.display import HTML , display from tabulate import tabulate import scipy.spatial.distance as spad def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = pd . read_csv ( 'outlier.csv' , usecols = [ 'user_id' , 'pause_video' , 'play_video' , 'seek_video' , 'stop_video' ], nrows = 20 ) table ( df ) user_id pause_video play_video seek_video stop_video 0 1 4 1 1 1 14 14 0 1 2 0 0 0 0 3 2 2 0 1 4 3 22 18 0 5 1 5 9 1 6 5 9 6 1 7 1 18 16 0 8 7 9 2 1 9 1 1 0 0 10 32 33 1 1 11 0 1 0 1 12 0 0 0 0 13 18 23 13 6 14 0 0 1 0 15 0 6 10 1 16 0 0 0 0 17 10 16 4 3 18 1 2 0 1 19 1 106 1 1 Outlier Detection Outlier adalah samples janggal yang keluar dari kerumuman. Mereka membuat integritas data tidak sehat. Suatu sampel A A dapat dikatakan sebagai outlier dalam data (D), jika $$ \\left(\\sum^n_{i=1}\\left[\\operatorname{dist}(A, D_i) > r\\right]\\right) > \\pi{n} $$ dimana r r adalah batas normal jarak dan \\pi \\pi adalah rasio toleransi (antara 0...1). Kedua r r dan \\pi \\pi dapat diatur secara empiris untuk mendapatkan data yang ideal r = 20 pi = 0.5 d = df . values def is_outlier ( i ): count = 0 n = len ( d ) for j in range ( n ): delta = spad . euclidean ( d [ i , 1 :], d [ j , 1 :]) if ( i != j and delta <= r ): count += 1 if count >= pi * n : return False return True print ( \"Deteksi outlier dengan r =\" , r , 'dan pi =' , pi ) table ( pd . DataFrame ([[ * d [ i ], 'Y' if is_outlier ( i ) else '-' ] for i in range ( len ( d ))], columns = [ * df . columns , \"Outliers?\" ])) Deteksi outlier dengan r = 20 dan pi = 0 . 5 user_id pause_video play_video seek_video stop_video Outliers? 0 1 4 1 1 - 1 14 14 0 1 - 2 0 0 0 0 - 3 2 2 0 1 - 4 3 22 18 0 Y 5 1 5 9 1 - 6 5 9 6 1 - 7 1 18 16 0 Y 8 7 9 2 1 - 9 1 1 0 0 - 10 32 33 1 1 Y 11 0 1 0 1 - 12 0 0 0 0 - 13 18 23 13 6 Y 14 0 0 1 0 - 15 0 6 10 1 - 16 0 0 0 0 - 17 10 16 4 3 - 18 1 2 0 1 - 19 1 106 1 1 Y Outliers Detection 2 Cara deteksi kedua (lebih efisien) adalah menghitung jarak dari mean setiap fitur ( c c ), sehingga sampel A A akan menjadi outlier jika \\left(\\sum^n_{i=1}\\frac{\\left(A_c-\\overline{c}\\right)^2}{\\overline{c}}\\right) > r \\left(\\sum^n_{i=1}\\frac{\\left(A_c-\\overline{c}\\right)^2}{\\overline{c}}\\right) > r # Outliers 2 avgs = [ df [ x ] . mean () for x in df . columns ][ 1 :] r = 50 d = df . values def get_is_outlier ( i ): dist = sum ([( c - avgs [ j ]) ** 2 / avgs [ j ] for j , c in enumerate ( d [ i , 1 :])]) return '{:.2f}' . format ( dist ), 'Y' if dist > r else '-' print ( \"Deteksi outlier dengan r =\" , r ) table ( pd . DataFrame ([[ * d [ i ], * get_is_outlier ( i )] for i in range ( len ( d ))], columns = [ * df . columns , \"Dist\" , \"Outliers?\" ])) Deteksi outlier dengan r = 50 user_id pause_video play_video seek_video stop_video Dist Outliers? 0 1 4 1 1 12.13 - 1 14 14 0 1 21.38 - 2 0 0 0 0 23.5 - 3 2 2 0 1 15.62 - 4 3 22 18 0 54.1 Y 5 1 5 9 1 14.31 - 6 5 9 6 1 2.41 - 7 1 18 16 0 40.06 - 8 7 9 2 1 3.56 - 9 1 1 0 0 19.78 - 10 32 33 1 1 182.25 Y 11 0 1 0 1 20.57 - 12 0 0 0 0 23.5 - 13 18 23 13 6 86.56 Y 14 0 0 1 0 21.74 - 15 0 6 10 1 17.55 - 16 0 0 0 0 23.5 - 17 10 16 4 3 9.91 - 18 1 2 0 1 17 - 19 1 106 1 1 636.18 Y Handling Missing Values with KNN KNN (K-Neighboring) from numpy import nan from sklearn.impute import KNNImputer dm = df . values . tolist () dm [ 6 ][ 2 ] = nan dm [ 9 ][ 3 ] = nan dfm = pd . DataFrame ( dm , columns = df . columns ) print ( \"Before\" ) table ( dfm ) imputer = KNNImputer ( n_neighbors = 5 ) dm = imputer . fit_transform ( dm ) dfm = pd . DataFrame ( dm , columns = df . columns ) print ( \"After\" ) table ( dfm ) Before user_id pause_video play_video seek_video stop_video 0 1 4 1 1 1 14 14 0 1 2 0 0 0 0 3 2 2 0 1 4 3 22 18 0 5 1 5 9 1 6 5 nan 6 1 7 1 18 16 0 8 7 9 2 1 9 1 1 nan 0 10 32 33 1 1 11 0 1 0 1 12 0 0 0 0 13 18 23 13 6 14 0 0 1 0 15 0 6 10 1 16 0 0 0 0 17 10 16 4 3 18 1 2 0 1 19 1 106 1 1 After user_id pause_video play_video seek_video stop_video 0 1 4 1 1 1 14 14 0 1 2 0 0 0 0 3 2 2 0 1 4 3 22 18 0 5 1 5 9 1 6 5 4.2 6 1 7 1 18 16 0 8 7 9 2 1 9 1 1 3.2 0 10 32 33 1 1 11 0 1 0 1 12 0 0 0 0 13 18 23 13 6 14 0 0 1 0 15 0 6 10 1 16 0 0 0 0 17 10 16 4 3 18 1 2 0 1 19 1 106 1 1","title":"Seleksi Sampel"},{"location":"PD_03_Seleksi_Sampel/#seleksi-sampel","text":"Mari kita buat beberapa sampel import pandas as pd import statistics , itertools from IPython.display import HTML , display from tabulate import tabulate import scipy.spatial.distance as spad def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = pd . read_csv ( 'outlier.csv' , usecols = [ 'user_id' , 'pause_video' , 'play_video' , 'seek_video' , 'stop_video' ], nrows = 20 ) table ( df ) user_id pause_video play_video seek_video stop_video 0 1 4 1 1 1 14 14 0 1 2 0 0 0 0 3 2 2 0 1 4 3 22 18 0 5 1 5 9 1 6 5 9 6 1 7 1 18 16 0 8 7 9 2 1 9 1 1 0 0 10 32 33 1 1 11 0 1 0 1 12 0 0 0 0 13 18 23 13 6 14 0 0 1 0 15 0 6 10 1 16 0 0 0 0 17 10 16 4 3 18 1 2 0 1 19 1 106 1 1","title":"Seleksi Sampel"},{"location":"PD_03_Seleksi_Sampel/#outlier-detection","text":"Outlier adalah samples janggal yang keluar dari kerumuman. Mereka membuat integritas data tidak sehat. Suatu sampel A A dapat dikatakan sebagai outlier dalam data (D), jika $$ \\left(\\sum^n_{i=1}\\left[\\operatorname{dist}(A, D_i) > r\\right]\\right) > \\pi{n} $$ dimana r r adalah batas normal jarak dan \\pi \\pi adalah rasio toleransi (antara 0...1). Kedua r r dan \\pi \\pi dapat diatur secara empiris untuk mendapatkan data yang ideal r = 20 pi = 0.5 d = df . values def is_outlier ( i ): count = 0 n = len ( d ) for j in range ( n ): delta = spad . euclidean ( d [ i , 1 :], d [ j , 1 :]) if ( i != j and delta <= r ): count += 1 if count >= pi * n : return False return True print ( \"Deteksi outlier dengan r =\" , r , 'dan pi =' , pi ) table ( pd . DataFrame ([[ * d [ i ], 'Y' if is_outlier ( i ) else '-' ] for i in range ( len ( d ))], columns = [ * df . columns , \"Outliers?\" ])) Deteksi outlier dengan r = 20 dan pi = 0 . 5 user_id pause_video play_video seek_video stop_video Outliers? 0 1 4 1 1 - 1 14 14 0 1 - 2 0 0 0 0 - 3 2 2 0 1 - 4 3 22 18 0 Y 5 1 5 9 1 - 6 5 9 6 1 - 7 1 18 16 0 Y 8 7 9 2 1 - 9 1 1 0 0 - 10 32 33 1 1 Y 11 0 1 0 1 - 12 0 0 0 0 - 13 18 23 13 6 Y 14 0 0 1 0 - 15 0 6 10 1 - 16 0 0 0 0 - 17 10 16 4 3 - 18 1 2 0 1 - 19 1 106 1 1 Y","title":"Outlier Detection"},{"location":"PD_03_Seleksi_Sampel/#outliers-detection-2","text":"Cara deteksi kedua (lebih efisien) adalah menghitung jarak dari mean setiap fitur ( c c ), sehingga sampel A A akan menjadi outlier jika \\left(\\sum^n_{i=1}\\frac{\\left(A_c-\\overline{c}\\right)^2}{\\overline{c}}\\right) > r \\left(\\sum^n_{i=1}\\frac{\\left(A_c-\\overline{c}\\right)^2}{\\overline{c}}\\right) > r # Outliers 2 avgs = [ df [ x ] . mean () for x in df . columns ][ 1 :] r = 50 d = df . values def get_is_outlier ( i ): dist = sum ([( c - avgs [ j ]) ** 2 / avgs [ j ] for j , c in enumerate ( d [ i , 1 :])]) return '{:.2f}' . format ( dist ), 'Y' if dist > r else '-' print ( \"Deteksi outlier dengan r =\" , r ) table ( pd . DataFrame ([[ * d [ i ], * get_is_outlier ( i )] for i in range ( len ( d ))], columns = [ * df . columns , \"Dist\" , \"Outliers?\" ])) Deteksi outlier dengan r = 50 user_id pause_video play_video seek_video stop_video Dist Outliers? 0 1 4 1 1 12.13 - 1 14 14 0 1 21.38 - 2 0 0 0 0 23.5 - 3 2 2 0 1 15.62 - 4 3 22 18 0 54.1 Y 5 1 5 9 1 14.31 - 6 5 9 6 1 2.41 - 7 1 18 16 0 40.06 - 8 7 9 2 1 3.56 - 9 1 1 0 0 19.78 - 10 32 33 1 1 182.25 Y 11 0 1 0 1 20.57 - 12 0 0 0 0 23.5 - 13 18 23 13 6 86.56 Y 14 0 0 1 0 21.74 - 15 0 6 10 1 17.55 - 16 0 0 0 0 23.5 - 17 10 16 4 3 9.91 - 18 1 2 0 1 17 - 19 1 106 1 1 636.18 Y","title":"Outliers Detection 2"},{"location":"PD_03_Seleksi_Sampel/#handling-missing-values-with-knn","text":"KNN (K-Neighboring) from numpy import nan from sklearn.impute import KNNImputer dm = df . values . tolist () dm [ 6 ][ 2 ] = nan dm [ 9 ][ 3 ] = nan dfm = pd . DataFrame ( dm , columns = df . columns ) print ( \"Before\" ) table ( dfm ) imputer = KNNImputer ( n_neighbors = 5 ) dm = imputer . fit_transform ( dm ) dfm = pd . DataFrame ( dm , columns = df . columns ) print ( \"After\" ) table ( dfm ) Before user_id pause_video play_video seek_video stop_video 0 1 4 1 1 1 14 14 0 1 2 0 0 0 0 3 2 2 0 1 4 3 22 18 0 5 1 5 9 1 6 5 nan 6 1 7 1 18 16 0 8 7 9 2 1 9 1 1 nan 0 10 32 33 1 1 11 0 1 0 1 12 0 0 0 0 13 18 23 13 6 14 0 0 1 0 15 0 6 10 1 16 0 0 0 0 17 10 16 4 3 18 1 2 0 1 19 1 106 1 1 After user_id pause_video play_video seek_video stop_video 0 1 4 1 1 1 14 14 0 1 2 0 0 0 0 3 2 2 0 1 4 3 22 18 0 5 1 5 9 1 6 5 4.2 6 1 7 1 18 16 0 8 7 9 2 1 9 1 1 3.2 0 10 32 33 1 1 11 0 1 0 1 12 0 0 0 0 13 18 23 13 6 14 0 0 1 0 15 0 6 10 1 16 0 0 0 0 17 10 16 4 3 18 1 2 0 1 19 1 106 1 1","title":"Handling Missing Values with KNN"},{"location":"PD_04_Seleksi_Fitur/","text":"Seleksi Fitur Kita dapat menghitung \"seberapa berharga\" fitur X dalam data melalui Feature Gain. Dengan demikian, fitur terlalu banyak bisa dikurangi. from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) Mari kita ambil beberapa sampel: df = read_csv ( 'play.csv' , sep = ';' ) table ( df ) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Entropy Target Entropy (keberagaman) kolom target: E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} dimana P P = Rasio Peluang muncul dalam record def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0 . 9402859586706309 Gain Gain dalam sebuah fitur X X untuk data T T : \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain dari ' %s ': %f \" % ( column , gain )) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain dari 'outlook' : 0 . 246750 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain dari 'temperature' : 0 . 029223 value count probability high 7 0.5 normal 7 0.5 gain dari 'humidity' : 0 . 151836 value count probability False 8 0.571429 True 6 0.428571 gain dari 'windy' : 0 . 048127 Overall Gain Score: result = DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ] table ( result ) print ( \"' %s ' mempunyai gain score tertinggi sedangkan ' %s ' terendah\" % ( result . values [ 0 , 0 ], result . values [ - 1 , 0 ])) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 'outlook' mempunyai gain score tertinggi sedangkan 'temperature' terendah","title":"Seleksi Fitur"},{"location":"PD_04_Seleksi_Fitur/#seleksi-fitur","text":"Kita dapat menghitung \"seberapa berharga\" fitur X dalam data melalui Feature Gain. Dengan demikian, fitur terlalu banyak bisa dikurangi. from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) Mari kita ambil beberapa sampel: df = read_csv ( 'play.csv' , sep = ';' ) table ( df ) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no","title":"Seleksi Fitur"},{"location":"PD_04_Seleksi_Fitur/#entropy-target","text":"Entropy (keberagaman) kolom target: E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} dimana P P = Rasio Peluang muncul dalam record def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0 . 9402859586706309","title":"Entropy Target"},{"location":"PD_04_Seleksi_Fitur/#gain","text":"Gain dalam sebuah fitur X X untuk data T T : \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain dari ' %s ': %f \" % ( column , gain )) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain dari 'outlook' : 0 . 246750 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain dari 'temperature' : 0 . 029223 value count probability high 7 0.5 normal 7 0.5 gain dari 'humidity' : 0 . 151836 value count probability False 8 0.571429 True 6 0.428571 gain dari 'windy' : 0 . 048127","title":"Gain"},{"location":"PD_04_Seleksi_Fitur/#overall-gain-score","text":"result = DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ] table ( result ) print ( \"' %s ' mempunyai gain score tertinggi sedangkan ' %s ' terendah\" % ( result . values [ 0 , 0 ], result . values [ - 1 , 0 ])) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 'outlook' mempunyai gain score tertinggi sedangkan 'temperature' terendah","title":"Overall Gain Score:"},{"location":"PD_05_Decision_Tree/","text":"Decision Tree #Contoh eksekusi from sklearn import datasets , model_selection from pandas import * from sklearn.metrics import classification_report , confusion_matrix , accuracy_score from sklearn.tree import DecisionTreeClassifier , plot_tree from sklearn.tree import export_graphviz import graphviz from IPython.display import HTML , display ; from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) iris = datasets . load_iris () data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) array = dataset . values X = array [:, 0 : 4 ] Y = array [:, 4 ] X_train , X_validation , Y_train , Y_validation = \\ model_selection . train_test_split ( X , Y , \\ train_size = 0.5 , random_state = 2 ) classifier = DecisionTreeClassifier () fitting = classifier . fit ( X_train , Y_train ) p = classifier . predict ( X_validation ) pdd = DataFrame ([ list ( X_validation [ i ]) + [ Y_validation [ i ], p [ i ]] for i in range ( X_validation . shape [ 0 ])]) table ( pdd ) print ( \"Akurasi: \" , accuracy_score ( Y_validation , p )) 0 1 2 3 4 5 4.6 3.4 1.4 0.3 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5.7 2.5 5 2 virginica virginica 4.8 3 1.4 0.1 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 7.2 3 5.8 1.6 virginica virginica 5 3 1.6 0.2 setosa setosa 6.7 2.5 5.8 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 4.8 3 1.4 0.3 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 6 3.4 4.5 1.6 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 4.5 2.3 1.3 0.3 setosa setosa 5.7 2.9 4.2 1.3 versicolor versicolor 6.7 3.3 5.7 2.5 virginica virginica 5.5 2.5 4 1.3 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6.4 2.9 4.3 1.3 versicolor versicolor 6.4 3.2 5.3 2.3 virginica virginica 5.6 2.7 4.2 1.3 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 4.7 3.2 1.6 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 6.1 3 4.9 1.8 virginica virginica 5.1 3.8 1.9 0.4 setosa setosa 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 5.1 3.3 1.7 0.5 setosa setosa 5.6 2.9 3.6 1.3 versicolor versicolor 7.7 3.8 6.7 2.2 virginica virginica 5.4 3 4.5 1.5 versicolor versicolor 5.8 4 1.2 0.2 setosa setosa 6.4 2.8 5.6 2.2 virginica virginica 6.1 3 4.6 1.4 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.9 3.1 5.1 2.3 virginica virginica 6 2.9 4.5 1.5 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 6.8 3.2 5.9 2.3 virginica virginica 5 2.3 3.3 1 versicolor versicolor 4.8 3.4 1.6 0.2 setosa setosa 6.1 2.6 5.6 1.4 virginica versicolor 5.2 3.4 1.4 0.2 setosa setosa 6.7 3.1 4.4 1.4 versicolor versicolor 5.1 3.5 1.4 0.2 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 2.5 4.5 1.7 virginica versicolor 6.2 3.4 5.4 2.3 virginica virginica 7.9 3.8 6.4 2 virginica virginica 5.4 3.4 1.7 0.2 setosa setosa 6.7 3.1 5.6 2.4 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 7.6 3 6.6 2.1 virginica virginica 6 2.2 5 1.5 virginica versicolor 4.3 3 1.1 0.1 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.8 2.7 5.1 1.9 virginica virginica 5.7 2.8 4.1 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.1 2.8 4 1.3 versicolor versicolor 5.1 3.7 1.5 0.4 setosa setosa 5.7 2.8 4.5 1.3 versicolor versicolor 5.4 3.9 1.3 0.4 setosa setosa 5.8 2.8 5.1 2.4 virginica virginica 5.8 2.6 4 1.2 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 3.8 1.7 0.3 setosa setosa 5.5 2.4 3.7 1 versicolor versicolor Akurasi : 0.9466666666666667 dot_data = export_graphviz ( classifier , out_file = None , feature_names = iris . feature_names , class_names = iris . target_names , filled = True , rounded = True , special_characters = True ) graphviz . Source ( dot_data )","title":"Decision Tree"},{"location":"PD_05_Decision_Tree/#decision-tree","text":"#Contoh eksekusi from sklearn import datasets , model_selection from pandas import * from sklearn.metrics import classification_report , confusion_matrix , accuracy_score from sklearn.tree import DecisionTreeClassifier , plot_tree from sklearn.tree import export_graphviz import graphviz from IPython.display import HTML , display ; from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) iris = datasets . load_iris () data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) array = dataset . values X = array [:, 0 : 4 ] Y = array [:, 4 ] X_train , X_validation , Y_train , Y_validation = \\ model_selection . train_test_split ( X , Y , \\ train_size = 0.5 , random_state = 2 ) classifier = DecisionTreeClassifier () fitting = classifier . fit ( X_train , Y_train ) p = classifier . predict ( X_validation ) pdd = DataFrame ([ list ( X_validation [ i ]) + [ Y_validation [ i ], p [ i ]] for i in range ( X_validation . shape [ 0 ])]) table ( pdd ) print ( \"Akurasi: \" , accuracy_score ( Y_validation , p )) 0 1 2 3 4 5 4.6 3.4 1.4 0.3 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5.7 2.5 5 2 virginica virginica 4.8 3 1.4 0.1 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 7.2 3 5.8 1.6 virginica virginica 5 3 1.6 0.2 setosa setosa 6.7 2.5 5.8 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 4.8 3 1.4 0.3 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 6 3.4 4.5 1.6 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 4.5 2.3 1.3 0.3 setosa setosa 5.7 2.9 4.2 1.3 versicolor versicolor 6.7 3.3 5.7 2.5 virginica virginica 5.5 2.5 4 1.3 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6.4 2.9 4.3 1.3 versicolor versicolor 6.4 3.2 5.3 2.3 virginica virginica 5.6 2.7 4.2 1.3 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 4.7 3.2 1.6 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 6.1 3 4.9 1.8 virginica virginica 5.1 3.8 1.9 0.4 setosa setosa 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 5.1 3.3 1.7 0.5 setosa setosa 5.6 2.9 3.6 1.3 versicolor versicolor 7.7 3.8 6.7 2.2 virginica virginica 5.4 3 4.5 1.5 versicolor versicolor 5.8 4 1.2 0.2 setosa setosa 6.4 2.8 5.6 2.2 virginica virginica 6.1 3 4.6 1.4 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.9 3.1 5.1 2.3 virginica virginica 6 2.9 4.5 1.5 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 6.8 3.2 5.9 2.3 virginica virginica 5 2.3 3.3 1 versicolor versicolor 4.8 3.4 1.6 0.2 setosa setosa 6.1 2.6 5.6 1.4 virginica versicolor 5.2 3.4 1.4 0.2 setosa setosa 6.7 3.1 4.4 1.4 versicolor versicolor 5.1 3.5 1.4 0.2 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 2.5 4.5 1.7 virginica versicolor 6.2 3.4 5.4 2.3 virginica virginica 7.9 3.8 6.4 2 virginica virginica 5.4 3.4 1.7 0.2 setosa setosa 6.7 3.1 5.6 2.4 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 7.6 3 6.6 2.1 virginica virginica 6 2.2 5 1.5 virginica versicolor 4.3 3 1.1 0.1 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.8 2.7 5.1 1.9 virginica virginica 5.7 2.8 4.1 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.1 2.8 4 1.3 versicolor versicolor 5.1 3.7 1.5 0.4 setosa setosa 5.7 2.8 4.5 1.3 versicolor versicolor 5.4 3.9 1.3 0.4 setosa setosa 5.8 2.8 5.1 2.4 virginica virginica 5.8 2.6 4 1.2 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 3.8 1.7 0.3 setosa setosa 5.5 2.4 3.7 1 versicolor versicolor Akurasi : 0.9466666666666667 dot_data = export_graphviz ( classifier , out_file = None , feature_names = iris . feature_names , class_names = iris . target_names , filled = True , rounded = True , special_characters = True ) graphviz . Source ( dot_data )","title":"Decision Tree"},{"location":"PD_06_Naive_Bayes/","text":"Naive Bayes Classifier Naive Bayes Classifier adalah classifier dimana untuk setiap fitur X X sejumlah n n : P(\\operatorname{C_k}) = \\frac{\\left(\\prod_{i=1}^n P(X_i|C_k)\\right)P(C_k)}{P(X)} P(\\operatorname{C_k}) = \\frac{\\left(\\prod_{i=1}^n P(X_i|C_k)\\right)P(C_k)}{P(X)} Layman terms: \\operatorname{posterior} = \\frac{\\operatorname{prior}\\times\\operatorname{likehood}}{\\operatorname{evidence}} \\operatorname{posterior} = \\frac{\\operatorname{prior}\\times\\operatorname{likehood}}{\\operatorname{evidence}} P P adalah probabilitas yang muncul. Untuk data numerik P P adalah: P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) dimana v v adalah nilai dalam fitur, \\sigma_k \\sigma_k adalah Standar deviasi dan \\mu_k \\mu_k adalah Rata-rata untuk K (kolom) Langkah-Langkah Training 1. Ambil data set from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML , display ; from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) # IRIS TRAINING TABLE iris = datasets . load_iris () data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) . sample ( frac = 0.2 ) table ( dataset ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa 2. Sampel data untuk di tes test = [ 3 , 5 , 2 , 4 ] print ( \"sampel data: \" , test ) sampel data : [ 3 , 5 , 2 , 4 ] 3. Identifikasi Per Grup Class Target untuk data Training dataset_classes = {} # table per classes for key , group in dataset . groupby ( 'class' ): mu_s = [ group [ c ] . mean () for c in group . columns [: - 1 ]] sigma_s = [ group [ c ] . std () for c in group . columns [: - 1 ]] dataset_classes [ key ] = [ group , mu_s , sigma_s ] print ( key , \"===>\" ) print ( 'Mu_s =>' , array ( mu_s )) print ( 'Sigma_s =>' , array ( sigma_s )) table ( group ) setosa ===> Mu_s => [ 5 . 18333333 3 . 56666667 1 . 51666667 0 . 31666667 ] Sigma_s => [ 0 . 3250641 0 . 22509257 0 . 14719601 0 . 1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [ 5 . 89166667 2 . 76666667 4 . 13333333 1 . 26666667 ] Sigma_s => [ 0 . 52476546 0 . 33393884 0 . 46188022 0 . 21461735 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [ 6 . 61666667 3 . 13333333 5 . 58333333 2 . 06666667 ] Sigma_s => [ 0 . 7790826 0 . 34465617 0 . 59670814 0 . 23868326 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica 5. Hitung Probabilitas Prior dan Likehood WIP: Probabilitas Evidence masukkan ke hitungan def numericalPriorProbability ( v , mu , sigma ): return ( 1.0 / sqrt ( 2 * pi * ( sigma ** 2 )) * exp ( - (( v - mu ) ** 2 ) / ( 2 * ( sigma ** 2 )))) def categoricalProbability ( sample , universe ): return sample . shape [ 0 ] / universe . shape [ 0 ] Ps = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( test )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) table ( DataFrame ( Ps , columns = [ \"classes\" ] + [ \"P( %d | C )\" % d for d in test ] + [ \"P( C )\" ])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4 6. Rank & Tarik Kesimpulan Pss = ([[ r [ 0 ], prod ( r [ 1 :])] for r in Ps ]) PDss = DataFrame ( Pss , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] table ( PDss ) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print ( \"Prediksi Bayes untuk\" , test , \"adalah\" , PDss . values [ 0 , 0 ]) Prediksi Bayes untuk [ 3 , 5 , 2 , 4 ] adalah virginica Real Test diberikan variabel dataset dan dataset_classes yang sebagai 'data training' kita, mari kita lakukan itu untuk real Iris data: # ONE FUNCTION FOR CLASSIFIER def predict ( sampel ): priorLikehoods = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( sampel )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) products = ([[ r [ 0 ], prod ( r [ 1 :])] for r in priorLikehoods ]) result = DataFrame ( products , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] return result . values [ 0 , 0 ] dataset_test = DataFrame ([ list ( d ) + [ predict ( d [: 4 ])] for d in data ], columns = list ( dataset . columns ) + [ 'predicted class (by predict())' ]) table ( dataset_test ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica Kesimpulan corrects = dataset_test . loc [ dataset_test [ 'class' ] == dataset_test [ 'predicted class (by predict())' ]] . shape [ 0 ] print ( 'Prediksi Training Bayes: %d of %d == %f %% ' % ( corrects , len ( data ), corrects / len ( data ) * 100 )) Prediksi Training Bayes : 144 of 150 == 96 . 000000 %","title":"Naive Bayes Classifier"},{"location":"PD_06_Naive_Bayes/#naive-bayes-classifier","text":"Naive Bayes Classifier adalah classifier dimana untuk setiap fitur X X sejumlah n n : P(\\operatorname{C_k}) = \\frac{\\left(\\prod_{i=1}^n P(X_i|C_k)\\right)P(C_k)}{P(X)} P(\\operatorname{C_k}) = \\frac{\\left(\\prod_{i=1}^n P(X_i|C_k)\\right)P(C_k)}{P(X)} Layman terms: \\operatorname{posterior} = \\frac{\\operatorname{prior}\\times\\operatorname{likehood}}{\\operatorname{evidence}} \\operatorname{posterior} = \\frac{\\operatorname{prior}\\times\\operatorname{likehood}}{\\operatorname{evidence}} P P adalah probabilitas yang muncul. Untuk data numerik P P adalah: P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) dimana v v adalah nilai dalam fitur, \\sigma_k \\sigma_k adalah Standar deviasi dan \\mu_k \\mu_k adalah Rata-rata untuk K (kolom)","title":"Naive Bayes Classifier"},{"location":"PD_06_Naive_Bayes/#langkah-langkah-training","text":"","title":"Langkah-Langkah Training"},{"location":"PD_06_Naive_Bayes/#1-ambil-data-set","text":"from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML , display ; from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) # IRIS TRAINING TABLE iris = datasets . load_iris () data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) . sample ( frac = 0.2 ) table ( dataset ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa","title":"1. Ambil data set"},{"location":"PD_06_Naive_Bayes/#2-sampel-data-untuk-di-tes","text":"test = [ 3 , 5 , 2 , 4 ] print ( \"sampel data: \" , test ) sampel data : [ 3 , 5 , 2 , 4 ]","title":"2. Sampel data untuk di tes"},{"location":"PD_06_Naive_Bayes/#3-identifikasi-per-grup-class-target-untuk-data-training","text":"dataset_classes = {} # table per classes for key , group in dataset . groupby ( 'class' ): mu_s = [ group [ c ] . mean () for c in group . columns [: - 1 ]] sigma_s = [ group [ c ] . std () for c in group . columns [: - 1 ]] dataset_classes [ key ] = [ group , mu_s , sigma_s ] print ( key , \"===>\" ) print ( 'Mu_s =>' , array ( mu_s )) print ( 'Sigma_s =>' , array ( sigma_s )) table ( group ) setosa ===> Mu_s => [ 5 . 18333333 3 . 56666667 1 . 51666667 0 . 31666667 ] Sigma_s => [ 0 . 3250641 0 . 22509257 0 . 14719601 0 . 1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [ 5 . 89166667 2 . 76666667 4 . 13333333 1 . 26666667 ] Sigma_s => [ 0 . 52476546 0 . 33393884 0 . 46188022 0 . 21461735 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [ 6 . 61666667 3 . 13333333 5 . 58333333 2 . 06666667 ] Sigma_s => [ 0 . 7790826 0 . 34465617 0 . 59670814 0 . 23868326 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica","title":"3. Identifikasi Per Grup Class Target untuk data Training"},{"location":"PD_06_Naive_Bayes/#5-hitung-probabilitas-prior-dan-likehood","text":"WIP: Probabilitas Evidence masukkan ke hitungan def numericalPriorProbability ( v , mu , sigma ): return ( 1.0 / sqrt ( 2 * pi * ( sigma ** 2 )) * exp ( - (( v - mu ) ** 2 ) / ( 2 * ( sigma ** 2 )))) def categoricalProbability ( sample , universe ): return sample . shape [ 0 ] / universe . shape [ 0 ] Ps = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( test )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) table ( DataFrame ( Ps , columns = [ \"classes\" ] + [ \"P( %d | C )\" % d for d in test ] + [ \"P( C )\" ])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4","title":"5. Hitung Probabilitas Prior dan Likehood"},{"location":"PD_06_Naive_Bayes/#6-rank-tarik-kesimpulan","text":"Pss = ([[ r [ 0 ], prod ( r [ 1 :])] for r in Ps ]) PDss = DataFrame ( Pss , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] table ( PDss ) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print ( \"Prediksi Bayes untuk\" , test , \"adalah\" , PDss . values [ 0 , 0 ]) Prediksi Bayes untuk [ 3 , 5 , 2 , 4 ] adalah virginica","title":"6. Rank &amp; Tarik Kesimpulan"},{"location":"PD_06_Naive_Bayes/#real-test","text":"diberikan variabel dataset dan dataset_classes yang sebagai 'data training' kita, mari kita lakukan itu untuk real Iris data: # ONE FUNCTION FOR CLASSIFIER def predict ( sampel ): priorLikehoods = ([[ y ] + [ numericalPriorProbability ( x , d [ 1 ][ i ], d [ 2 ][ i ]) for i , x in enumerate ( sampel )] + [ categoricalProbability ( d [ 0 ], dataset )] for y , d in dataset_classes . items ()]) products = ([[ r [ 0 ], prod ( r [ 1 :])] for r in priorLikehoods ]) result = DataFrame ( products , columns = [ 'class' , 'probability' ]) . sort_values ( 'probability' )[:: - 1 ] return result . values [ 0 , 0 ] dataset_test = DataFrame ([ list ( d ) + [ predict ( d [: 4 ])] for d in data ], columns = list ( dataset . columns ) + [ 'predicted class (by predict())' ]) table ( dataset_test ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica","title":"Real Test"},{"location":"PD_06_Naive_Bayes/#kesimpulan","text":"corrects = dataset_test . loc [ dataset_test [ 'class' ] == dataset_test [ 'predicted class (by predict())' ]] . shape [ 0 ] print ( 'Prediksi Training Bayes: %d of %d == %f %% ' % ( corrects , len ( data ), corrects / len ( data ) * 100 )) Prediksi Training Bayes : 144 of 150 == 96 . 000000 %","title":"Kesimpulan"},{"location":"PD_07_KNN/","text":"K-NN K-NN: K-Nearest Neighborhood WIP Weighted KNN Download Sheet Example Iframe sandboxed","title":"K-NN"},{"location":"PD_07_KNN/#k-nn","text":"K-NN: K-Nearest Neighborhood WIP","title":"K-NN"},{"location":"PD_07_KNN/#weighted-knn","text":"Download Sheet Example Iframe sandboxed","title":"Weighted KNN"},{"location":"PD_08_Clustering/","text":"Clustering Clustering == Pengelompokan K-Mean Clustering K-Modes Clustering K-Prototype Clustering WIP","title":"Clustering"},{"location":"PD_08_Clustering/#clustering","text":"Clustering == Pengelompokan","title":"Clustering"},{"location":"PD_08_Clustering/#k-mean-clustering","text":"","title":"K-Mean Clustering"},{"location":"PD_08_Clustering/#k-modes-clustering","text":"","title":"K-Modes Clustering"},{"location":"PD_08_Clustering/#k-prototype-clustering","text":"WIP","title":"K-Prototype Clustering"},{"location":"PD_09_Fuzzy_CM/","text":"Fuzzy C-Mean Clustering Fuzzy adalah clustering menggunakan derajat keanggotaan dengan pendekatan incremental. Steps 1. Persiapkan Environment: from pandas import DataFrame import random import numpy as np from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) 2. Persiapkan Input Data (D = m x n): n : Jumlah Sampel m : Jumlah Fitur c : Jumlah Cluster w : Tingkat blur/fuzzy (biasanya 2) T : Batas maks Iterasi (biasanya 10) e : Akurasi (biasanya 0.1) Pt : Fungsi Objektif ke-t t : Iterasi ke-t Data = read_csv ( 'leaf.csv' , sep = ',' ) Data = Data [[ 'Eccentricity' , 'Solidity' , 'Lobedness' , 'Entropy' ]] . sample ( 6 , random_state = 42 ) D = Data . values print ( \"Table (D) >>\" ) table ( D ) Table ( D ) >> 0 1 2 3 0.99593 0.80662 2.7342 0.27303 0.50692 0.53024 3.0788 0.67289 0.24465 0.56524 2.854 0.8331 0.86545 0.82443 0.40204 1.0136 0.82866 0.9418 0.11857 1.8038 0.72719 0.99388 0.0016019 0.9805 n , m , c , w , T , e , P0 , t = * D . shape , 3 , 2 , 10 , 0.1 , 0 , 1 print ( \"Variables >>\" ) print ( \" n = %d \\n m = %d \\n c = %d \\n w = %d \\n T = %d \\n e = %f \\n P0 = %d \\n t = %d \" % ( n , m , c , w , T , e , P0 , t )) Variables >> n = 6 m = 4 c = 3 w = 2 T = 10 e = 0 . 100000 P0 = 0 t = 1 3. Siapkan Matrik Derajat Kluster (U = c x n): Data diisi dengan random atau hasil iterasi lama random . seed ( 42 ) U = np . array ([[ random . uniform ( 0 , 1 ) for _ in range ( c )] for _ in range ( n )]) print ( \"U >> \\n \" ) print ( U ) U >> [[ 0 . 6394268 0 . 02501076 0 . 27502932 ] [ 0 . 22321074 0 . 73647121 0 . 67669949 ] [ 0 . 89217957 0 . 08693883 0 . 42192182 ] [ 0 . 02979722 0 . 21863797 0 . 50535529 ] [ 0 . 02653597 0 . 19883765 0 . 64988444 ] [ 0 . 54494148 0 . 22044062 0 . 58926568 ]] 4. Hitung Centroid Tiap Cluster (V = m x c): V_{xy} = \\frac{\\sum^n_{i=1}(U_{iy})^w\\times{D_{ix}}}{\\sum^n_{i=1}(U_{iy})^w} V_{xy} = \\frac{\\sum^n_{i=1}(U_{iy})^w\\times{D_{ix}}}{\\sum^n_{i=1}(U_{iy})^w} # Caution: NP Array is math-agnostic (column-by-column) def cluster ( U , D , x , y ): return sum ([ U [ i , y ] ** w * D [ i , x ] for i in range ( n )]) / sum ([ U [ i , y ] ** w for i in range ( n )]) V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) print ( \"V >> \\n \" ) print ( V ) V >> [[ 0 . 54370379 0 . 70992788 2 . 28168401 0 . 7092545 ] [ 0 . 56356398 0 . 60788268 2 . 50132412 0 . 78491767 ] [ 0 . 67635682 0 . 7819355 1 . 31182068 1 . 05856209 ]] 5. Hitung Fungsi Objektif pada t (Pt) P_t = \\sum^n_{i=1}\\sum^c_{k=1}\\left(\\left[\\sum^m_{j=1}\\left(D_{ij}-V_{kj}\\right)^2\\right](U_{ik})^w\\right) P_t = \\sum^n_{i=1}\\sum^c_{k=1}\\left(\\left[\\sum^m_{j=1}\\left(D_{ij}-V_{kj}\\right)^2\\right](U_{ik})^w\\right) def objective ( V , U , D ): return sum ([ sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) * ( U [ i , k ] ** w ) for k in range ( c )]) for i in range ( n )]) Pt = objective ( V , U , D ) print ( \"Pt >> \\n \" ) print ( Pt ) Pt >> 7 . 165764247017886 6. Hitung Ulang Matrik Derajat Kluster (U = c x n): U_{ik} = \\frac{\\left[\\sum^m_{j=1}(D_{ij}-V_{kj})^2\\right]^{\\frac{-1}{w-1}}}{\\sum^c_{k=1}\\left[\\sum^m_{j=1}(D_{ij}-V_{kj})^2\\right]^{\\frac{-1}{w-1}}} U_{ik} = \\frac{\\left[\\sum^m_{j=1}(D_{ij}-V_{kj})^2\\right]^{\\frac{-1}{w-1}}}{\\sum^c_{k=1}\\left[\\sum^m_{j=1}(D_{ij}-V_{kj})^2\\right]^{\\frac{-1}{w-1}}} def converge ( V , D , i , k ): return ( sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) ** ( - 1 / ( w - 1 ))) / sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) ** ( - 1 / ( w - 1 )) for k in range ( c )]) print ( \"U >> \\n \" ) np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]) array ([[ 0 . 42661745 , 0 . 47867606 , 0 . 09470648 ], [ 0 . 32401778 , 0 . 61139512 , 0 . 0645871 ], [ 0 . 31857727 , 0 . 62718924 , 0 . 05423349 ], [ 0 . 16315857 , 0 . 13281473 , 0 . 7040267 ], [ 0 . 20677417 , 0 . 18023246 , 0 . 61299337 ], [ 0 . 20507176 , 0 . 17092863 , 0 . 62399961 ]]) 7. Cek Berhenti Atau Loop Kembali Jika $ P_t - P_{t-1} < e $ atau $ t >= T $ maka BERHENTI Jika tidak, ulangi langkah dari Hitung Centroid Tiap Cluster def iterate ( U ): V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) return np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]), objective ( V , U , D ) def fuzzyCM ( U ): #U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U , P2 , P , t = * iterate ( U ), 0 , 1 while abs ( P2 - P ) > e and t < T : U , P2 , P , t = * iterate ( U ), P2 , t + 1 return U , t FuzzyResult , FuzzyIters = fuzzyCM ( U ) print ( \"Iterating %d times, fuzzy result >> \\n \" % FuzzyIters ) print ( FuzzyResult ) Iterating 5 times , fuzzy result >> [[ 9 . 99946808 e - 01 4 . 84760420 e - 05 4 . 71615530 e - 06 ] [ 5 . 97625571 e - 02 9 . 36333265 e - 01 3 . 90417747 e - 03 ] [ 3 . 61938911 e - 02 9 . 59438085 e - 01 4 . 36802436 e - 03 ] [ 1 . 97778963 e - 02 1 . 70437266 e - 02 9 . 63178377 e - 01 ] [ 3 . 11702255 e - 02 3 . 00196318 e - 02 9 . 38810143 e - 01 ] [ 1 . 40843238 e - 02 1 . 23997912 e - 02 9 . 73515885 e - 01 ]] 8. Ambil Nilai Terbesar pada Kolom Sebagai Cluster pada setiap Record Data table ( DataFrame ([ D [ i ] . tolist () + [ np . argmax ( FuzzyResult [ i ] . tolist ())] for i in range ( n )], columns = Data . columns . tolist () + [ \"Cluster Index\" ])) Eccentricity Solidity Lobedness Entropy Cluster Index 0.99593 0.80662 2.7342 0.27303 0 0.50692 0.53024 3.0788 0.67289 1 0.24465 0.56524 2.854 0.8331 1 0.86545 0.82443 0.40204 1.0136 2 0.82866 0.9418 0.11857 1.8038 2 0.72719 0.99388 0.0016019 0.9805 2","title":"Fuzzy C-Mean Clustering"},{"location":"PD_09_Fuzzy_CM/#fuzzy-c-mean-clustering","text":"Fuzzy adalah clustering menggunakan derajat keanggotaan dengan pendekatan incremental.","title":"Fuzzy C-Mean Clustering"},{"location":"PD_09_Fuzzy_CM/#steps","text":"","title":"Steps"},{"location":"PD_09_Fuzzy_CM/#1-persiapkan-environment","text":"from pandas import DataFrame import random import numpy as np from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False )))","title":"1. Persiapkan Environment:"},{"location":"PD_09_Fuzzy_CM/#2-persiapkan-input-data-d-m-x-n","text":"n : Jumlah Sampel m : Jumlah Fitur c : Jumlah Cluster w : Tingkat blur/fuzzy (biasanya 2) T : Batas maks Iterasi (biasanya 10) e : Akurasi (biasanya 0.1) Pt : Fungsi Objektif ke-t t : Iterasi ke-t Data = read_csv ( 'leaf.csv' , sep = ',' ) Data = Data [[ 'Eccentricity' , 'Solidity' , 'Lobedness' , 'Entropy' ]] . sample ( 6 , random_state = 42 ) D = Data . values print ( \"Table (D) >>\" ) table ( D ) Table ( D ) >> 0 1 2 3 0.99593 0.80662 2.7342 0.27303 0.50692 0.53024 3.0788 0.67289 0.24465 0.56524 2.854 0.8331 0.86545 0.82443 0.40204 1.0136 0.82866 0.9418 0.11857 1.8038 0.72719 0.99388 0.0016019 0.9805 n , m , c , w , T , e , P0 , t = * D . shape , 3 , 2 , 10 , 0.1 , 0 , 1 print ( \"Variables >>\" ) print ( \" n = %d \\n m = %d \\n c = %d \\n w = %d \\n T = %d \\n e = %f \\n P0 = %d \\n t = %d \" % ( n , m , c , w , T , e , P0 , t )) Variables >> n = 6 m = 4 c = 3 w = 2 T = 10 e = 0 . 100000 P0 = 0 t = 1","title":"2. Persiapkan Input Data (D = m x n):"},{"location":"PD_09_Fuzzy_CM/#3-siapkan-matrik-derajat-kluster-u-c-x-n","text":"Data diisi dengan random atau hasil iterasi lama random . seed ( 42 ) U = np . array ([[ random . uniform ( 0 , 1 ) for _ in range ( c )] for _ in range ( n )]) print ( \"U >> \\n \" ) print ( U ) U >> [[ 0 . 6394268 0 . 02501076 0 . 27502932 ] [ 0 . 22321074 0 . 73647121 0 . 67669949 ] [ 0 . 89217957 0 . 08693883 0 . 42192182 ] [ 0 . 02979722 0 . 21863797 0 . 50535529 ] [ 0 . 02653597 0 . 19883765 0 . 64988444 ] [ 0 . 54494148 0 . 22044062 0 . 58926568 ]]","title":"3. Siapkan Matrik Derajat Kluster (U = c x n):"},{"location":"PD_09_Fuzzy_CM/#4-hitung-centroid-tiap-cluster-v-m-x-c","text":"V_{xy} = \\frac{\\sum^n_{i=1}(U_{iy})^w\\times{D_{ix}}}{\\sum^n_{i=1}(U_{iy})^w} V_{xy} = \\frac{\\sum^n_{i=1}(U_{iy})^w\\times{D_{ix}}}{\\sum^n_{i=1}(U_{iy})^w} # Caution: NP Array is math-agnostic (column-by-column) def cluster ( U , D , x , y ): return sum ([ U [ i , y ] ** w * D [ i , x ] for i in range ( n )]) / sum ([ U [ i , y ] ** w for i in range ( n )]) V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) print ( \"V >> \\n \" ) print ( V ) V >> [[ 0 . 54370379 0 . 70992788 2 . 28168401 0 . 7092545 ] [ 0 . 56356398 0 . 60788268 2 . 50132412 0 . 78491767 ] [ 0 . 67635682 0 . 7819355 1 . 31182068 1 . 05856209 ]]","title":"4. Hitung Centroid Tiap Cluster (V = m x c):"},{"location":"PD_09_Fuzzy_CM/#5-hitung-fungsi-objektif-pada-t-pt","text":"P_t = \\sum^n_{i=1}\\sum^c_{k=1}\\left(\\left[\\sum^m_{j=1}\\left(D_{ij}-V_{kj}\\right)^2\\right](U_{ik})^w\\right) P_t = \\sum^n_{i=1}\\sum^c_{k=1}\\left(\\left[\\sum^m_{j=1}\\left(D_{ij}-V_{kj}\\right)^2\\right](U_{ik})^w\\right) def objective ( V , U , D ): return sum ([ sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) * ( U [ i , k ] ** w ) for k in range ( c )]) for i in range ( n )]) Pt = objective ( V , U , D ) print ( \"Pt >> \\n \" ) print ( Pt ) Pt >> 7 . 165764247017886","title":"5. Hitung Fungsi Objektif pada t (Pt)"},{"location":"PD_09_Fuzzy_CM/#6-hitung-ulang-matrik-derajat-kluster-u-c-x-n","text":"U_{ik} = \\frac{\\left[\\sum^m_{j=1}(D_{ij}-V_{kj})^2\\right]^{\\frac{-1}{w-1}}}{\\sum^c_{k=1}\\left[\\sum^m_{j=1}(D_{ij}-V_{kj})^2\\right]^{\\frac{-1}{w-1}}} U_{ik} = \\frac{\\left[\\sum^m_{j=1}(D_{ij}-V_{kj})^2\\right]^{\\frac{-1}{w-1}}}{\\sum^c_{k=1}\\left[\\sum^m_{j=1}(D_{ij}-V_{kj})^2\\right]^{\\frac{-1}{w-1}}} def converge ( V , D , i , k ): return ( sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) ** ( - 1 / ( w - 1 ))) / sum ([ sum ([( D [ i , j ] - V [ k , j ]) ** 2 for j in range ( m )]) ** ( - 1 / ( w - 1 )) for k in range ( c )]) print ( \"U >> \\n \" ) np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]) array ([[ 0 . 42661745 , 0 . 47867606 , 0 . 09470648 ], [ 0 . 32401778 , 0 . 61139512 , 0 . 0645871 ], [ 0 . 31857727 , 0 . 62718924 , 0 . 05423349 ], [ 0 . 16315857 , 0 . 13281473 , 0 . 7040267 ], [ 0 . 20677417 , 0 . 18023246 , 0 . 61299337 ], [ 0 . 20507176 , 0 . 17092863 , 0 . 62399961 ]])","title":"6. Hitung Ulang Matrik Derajat Kluster (U = c x n):"},{"location":"PD_09_Fuzzy_CM/#7-cek-berhenti-atau-loop-kembali","text":"Jika $ P_t - P_{t-1} < e $ atau $ t >= T $ maka BERHENTI Jika tidak, ulangi langkah dari Hitung Centroid Tiap Cluster def iterate ( U ): V = np . array ([[ cluster ( U , D , x , y ) for x in range ( m )] for y in range ( c )]) return np . array ([[ converge ( V , D , i , k ) for k in range ( c )] for i in range ( n )]), objective ( V , U , D ) def fuzzyCM ( U ): #U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U , P2 , P , t = * iterate ( U ), 0 , 1 while abs ( P2 - P ) > e and t < T : U , P2 , P , t = * iterate ( U ), P2 , t + 1 return U , t FuzzyResult , FuzzyIters = fuzzyCM ( U ) print ( \"Iterating %d times, fuzzy result >> \\n \" % FuzzyIters ) print ( FuzzyResult ) Iterating 5 times , fuzzy result >> [[ 9 . 99946808 e - 01 4 . 84760420 e - 05 4 . 71615530 e - 06 ] [ 5 . 97625571 e - 02 9 . 36333265 e - 01 3 . 90417747 e - 03 ] [ 3 . 61938911 e - 02 9 . 59438085 e - 01 4 . 36802436 e - 03 ] [ 1 . 97778963 e - 02 1 . 70437266 e - 02 9 . 63178377 e - 01 ] [ 3 . 11702255 e - 02 3 . 00196318 e - 02 9 . 38810143 e - 01 ] [ 1 . 40843238 e - 02 1 . 23997912 e - 02 9 . 73515885 e - 01 ]]","title":"7. Cek Berhenti Atau Loop Kembali"},{"location":"PD_09_Fuzzy_CM/#8-ambil-nilai-terbesar-pada-kolom-sebagai-cluster-pada-setiap-record-data","text":"table ( DataFrame ([ D [ i ] . tolist () + [ np . argmax ( FuzzyResult [ i ] . tolist ())] for i in range ( n )], columns = Data . columns . tolist () + [ \"Cluster Index\" ])) Eccentricity Solidity Lobedness Entropy Cluster Index 0.99593 0.80662 2.7342 0.27303 0 0.50692 0.53024 3.0788 0.67289 1 0.24465 0.56524 2.854 0.8331 1 0.86545 0.82443 0.40204 1.0136 2 0.82866 0.9418 0.11857 1.8038 2 0.72719 0.99388 0.0016019 0.9805 2","title":"8. Ambil Nilai Terbesar pada Kolom Sebagai Cluster pada setiap Record Data"},{"location":"DPWD/DPWD_TSP/","text":"Traveling Salesman Problem Optimization attempts from numpy import transpose , array from matplotlib.pyplot import scatter , subplots from scipy.spatial import distance_matrix from pandas import DataFrame points = array ([( 0 , 0 ), ( 0 , 4 ), ( 3 , 0 ), ( 1 , 1 ), ( 5 , 5 )]) labels = [ 'A' , 'B' , 'C' , 'D' , 'E' ] x , y = transpose ( points ) fig , ax = subplots () ax . scatter ( x , y ) for i , txt in enumerate ( labels ): ax . annotate ( txt , ( x [ i ], y [ i ])) df = DataFrame ( distance_matrix ( points , points ), columns = labels ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 0 0.000000 4.000000 3.000000 1.414214 7.071068 1 4.000000 0.000000 5.000000 3.162278 5.099020 2 3.000000 5.000000 0.000000 2.236068 5.385165 3 1.414214 3.162278 2.236068 0.000000 5.656854 4 7.071068 5.099020 5.385165 5.656854 0.000000 import math import itertools def isRouteValid ( routes ): counts = {} for f , t , d in routes : f = int ( f ); t = int ( t ) if ( f == t ): return False counts [ f ] = 1 if counts . get ( f , None ) is None else counts [ f ] + 1 counts [ t ] = 1 if counts . get ( t , None ) is None else counts [ t ] + 1 for c in counts : if ( counts [ c ] != 2 ): return False return True def mapEnroute ( dists , route ): r = [] for i in range ( len ( route )): r . append ( dists [ i ][ route [ i ]]) return r def nicePrint ( pds , i ): m = [ pds [ x ][ y ] for x , y in enumerate ( i )] label = lambda x : labels [ int ( x )] s = sum ([ d for f , t , d in m ]) return str ([ \"{0} -> {1} {2:.2f}\" . format ( label ( f ), label ( t ), d ) for f , t , d in m ]) + \" {:.2f}\" . format ( s ) def cost ( distances , labels ): dists = [] for x , row in enumerate ( distances ): for y , cell in enumerate ( row ): dists . append ([ x , y , cell ]) pd = DataFrame ( dists , columns = [ 'f' , 't' , 'd' ]) pds = [ list ( data . sort_values ( 'd' ) . values ) for key , data in pd . groupby ( 't' )] iters = list ( itertools . product ( * ([ list ( range ( len ( labels )))] * len ( labels )))) iters . sort ( key = lambda x : sum ( x )) for i in ( iters ): if ( isRouteValid ( mapEnroute ( pds , i ))): return nicePrint ( pds , i ) return 0 def bruteIt ( distances , labels ): dists = [] for x , row in enumerate ( distances ): for y , cell in enumerate ( row ): dists . append ([ x , y , cell ]) pd = DataFrame ( dists , columns = [ 'f' , 't' , 'd' ]) pds = [ list ( data . sort_values ( 'd' ) . values ) for key , data in pd . groupby ( 't' )] iters = list ( itertools . permutations ( list ( range ( len ( labels ))))) shortest_route = None shortest_dist = 999999999999999999 for i in iters : if any ([ x == y for x , y in enumerate ( i )]): continue distance = sum ([ distances [ x , y ] for x , y in enumerate ( i )]) if distance < shortest_dist : shortest_dist = distance shortest_route = i return shortest_dist , [ \"{}{} {:.2f}\" . format ( labels [ y ], labels [ x ], distances [ x , y ]) for x , y in enumerate ( shortest_route )], shortest_dist from timeit import default_timer as timer start = timer () print ( cost ( df . values , labels )) end = timer () print ( end - start , \"s\" ) start = timer () print ( bruteIt ( df . values , labels )) end = timer () print ( end - start , \"s\" ) [ 'C -> A 3.00' , 'D -> B 3.16' , 'E -> C 5.39' , 'A -> D 1.41' , 'B -> E 5.10' ] 18 . 06 0 . 06618459999981496 s ( 16 . 84832056705845 , [ 'DA 1.41' , 'EB 5.10' , 'AC 3.00' , 'CD 2.24' , 'BE 5.10' ], 16 . 84832056705845 ) 0 . 011839300000019648 s All code in GPL","title":"Traveling Salesman Problem"},{"location":"DPWD/DPWD_TSP/#traveling-salesman-problem","text":"Optimization attempts from numpy import transpose , array from matplotlib.pyplot import scatter , subplots from scipy.spatial import distance_matrix from pandas import DataFrame points = array ([( 0 , 0 ), ( 0 , 4 ), ( 3 , 0 ), ( 1 , 1 ), ( 5 , 5 )]) labels = [ 'A' , 'B' , 'C' , 'D' , 'E' ] x , y = transpose ( points ) fig , ax = subplots () ax . scatter ( x , y ) for i , txt in enumerate ( labels ): ax . annotate ( txt , ( x [ i ], y [ i ])) df = DataFrame ( distance_matrix ( points , points ), columns = labels ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 0 0.000000 4.000000 3.000000 1.414214 7.071068 1 4.000000 0.000000 5.000000 3.162278 5.099020 2 3.000000 5.000000 0.000000 2.236068 5.385165 3 1.414214 3.162278 2.236068 0.000000 5.656854 4 7.071068 5.099020 5.385165 5.656854 0.000000 import math import itertools def isRouteValid ( routes ): counts = {} for f , t , d in routes : f = int ( f ); t = int ( t ) if ( f == t ): return False counts [ f ] = 1 if counts . get ( f , None ) is None else counts [ f ] + 1 counts [ t ] = 1 if counts . get ( t , None ) is None else counts [ t ] + 1 for c in counts : if ( counts [ c ] != 2 ): return False return True def mapEnroute ( dists , route ): r = [] for i in range ( len ( route )): r . append ( dists [ i ][ route [ i ]]) return r def nicePrint ( pds , i ): m = [ pds [ x ][ y ] for x , y in enumerate ( i )] label = lambda x : labels [ int ( x )] s = sum ([ d for f , t , d in m ]) return str ([ \"{0} -> {1} {2:.2f}\" . format ( label ( f ), label ( t ), d ) for f , t , d in m ]) + \" {:.2f}\" . format ( s ) def cost ( distances , labels ): dists = [] for x , row in enumerate ( distances ): for y , cell in enumerate ( row ): dists . append ([ x , y , cell ]) pd = DataFrame ( dists , columns = [ 'f' , 't' , 'd' ]) pds = [ list ( data . sort_values ( 'd' ) . values ) for key , data in pd . groupby ( 't' )] iters = list ( itertools . product ( * ([ list ( range ( len ( labels )))] * len ( labels )))) iters . sort ( key = lambda x : sum ( x )) for i in ( iters ): if ( isRouteValid ( mapEnroute ( pds , i ))): return nicePrint ( pds , i ) return 0 def bruteIt ( distances , labels ): dists = [] for x , row in enumerate ( distances ): for y , cell in enumerate ( row ): dists . append ([ x , y , cell ]) pd = DataFrame ( dists , columns = [ 'f' , 't' , 'd' ]) pds = [ list ( data . sort_values ( 'd' ) . values ) for key , data in pd . groupby ( 't' )] iters = list ( itertools . permutations ( list ( range ( len ( labels ))))) shortest_route = None shortest_dist = 999999999999999999 for i in iters : if any ([ x == y for x , y in enumerate ( i )]): continue distance = sum ([ distances [ x , y ] for x , y in enumerate ( i )]) if distance < shortest_dist : shortest_dist = distance shortest_route = i return shortest_dist , [ \"{}{} {:.2f}\" . format ( labels [ y ], labels [ x ], distances [ x , y ]) for x , y in enumerate ( shortest_route )], shortest_dist from timeit import default_timer as timer start = timer () print ( cost ( df . values , labels )) end = timer () print ( end - start , \"s\" ) start = timer () print ( bruteIt ( df . values , labels )) end = timer () print ( end - start , \"s\" ) [ 'C -> A 3.00' , 'D -> B 3.16' , 'E -> C 5.39' , 'A -> D 1.41' , 'B -> E 5.10' ] 18 . 06 0 . 06618459999981496 s ( 16 . 84832056705845 , [ 'DA 1.41' , 'EB 5.10' , 'AC 3.00' , 'CD 2.24' , 'BE 5.10' ], 16 . 84832056705845 ) 0 . 011839300000019648 s All code in GPL","title":"Traveling Salesman Problem"},{"location":"DPWD/DWPD_Iris/","text":"Iris Training Iris flower data set, Input: Output: from sklearn import datasets from pandas import * import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix from numpy import array from sklearn import model_selection from sklearn.metrics import classification_report , confusion_matrix , accuracy_score from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from IPython.display import HTML , display from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) # IRIS iris = datasets . load_iris () print ( iris . DESCR ) .. _iris_dataset : Iris plants dataset -------------------- ** Data Set Characteristics : ** : Number of Instances : 150 ( 50 in each of three classes ) : Number of Attributes : 4 numeric , predictive attributes and the class : Attribute Information : - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class : - Iris - Setosa - Iris - Versicolour - Iris - Virginica : Summary Statistics : ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length : 4 . 3 7 . 9 5 . 84 0 . 83 0 . 7826 sepal width : 2 . 0 4 . 4 3 . 05 0 . 43 - 0 . 4194 petal length : 1 . 0 6 . 9 3 . 76 1 . 76 0 . 9490 ( high ! ) petal width : 0 . 1 2 . 5 1 . 20 0 . 76 0 . 9565 ( high ! ) ============== ==== ==== ======= ===== ==================== : Missing Attribute Values : None : Class Distribution : 33 . 3 % for each of 3 classes . : Creator : R . A . Fisher : Donor : Michael Marshall ( MARSHALL % PLU @ io . arc . nasa . gov ) : Date : July , 1988 The famous Iris database , first used by Sir R . A . Fisher . The dataset is taken from Fisher ' s paper. Note that it ' s the same as in R , but not as in the UCI Machine Learning Repository , which has two wrong data points . This is perhaps the best known database to be found in the pattern recognition literature . Fisher ' s paper is a classic in the field and is referenced frequently to this day . ( See Duda & Hart , for example . ) The data set contains 3 classes of 50 instances each , where each class refers to a type of iris plant . One class is linearly separable from the other 2 ; the latter are NOT linearly separable from each other . .. topic :: References - Fisher , R . A . \" The use of multiple measurements in taxonomic problems \" Annual Eugenics , 7 , Part II , 179 - 188 ( 1936 ) ; also in \"Contributions to Mathematical Statistics \" (John Wiley, NY, 1950). - Duda , R . O ., & Hart , P . E . ( 1973 ) Pattern Classification and Scene Analysis . ( Q327 . D83 ) John Wiley & Sons . ISBN 0 - 471 - 22361 - 1 . See page 218 . - Dasarathy , B . V . ( 1980 ) \" Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments \" . IEEE Transactions on Pattern Analysis and Machine Intelligence , Vol . PAMI - 2 , No . 1 , 67 - 71 . - Gates , G . W . ( 1972 ) \" The Reduced Nearest Neighbor Rule \" . IEEE Transactions on Information Theory , May 1972 , 431 - 433 . - See also : 1988 MLC Proceedings , 54 - 64 . Cheeseman et al \" s AUTOCLASS II conceptual clustering system finds 3 classes in the data . - Many , many more ... # IRIS TRAINING OUTPUT data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) table ( dataset ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.1 3.5 1.4 0.2 setosa 4.9 3 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3 1.4 0.1 setosa 4.3 3 1.1 0.1 setosa 5.8 4 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa 5.4 3.4 1.7 0.2 setosa 5.1 3.7 1.5 0.4 setosa 4.6 3.6 1 0.2 setosa 5.1 3.3 1.7 0.5 setosa 4.8 3.4 1.9 0.2 setosa 5 3 1.6 0.2 setosa 5 3.4 1.6 0.4 setosa 5.2 3.5 1.5 0.2 setosa 5.2 3.4 1.4 0.2 setosa 4.7 3.2 1.6 0.2 setosa 4.8 3.1 1.6 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 5.5 4.2 1.4 0.2 setosa 4.9 3.1 1.5 0.2 setosa 5 3.2 1.2 0.2 setosa 5.5 3.5 1.3 0.2 setosa 4.9 3.6 1.4 0.1 setosa 4.4 3 1.3 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5 3.5 1.3 0.3 setosa 4.5 2.3 1.3 0.3 setosa 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 5.1 3.8 1.9 0.4 setosa 4.8 3 1.4 0.3 setosa 5.1 3.8 1.6 0.2 setosa 4.6 3.2 1.4 0.2 setosa 5.3 3.7 1.5 0.2 setosa 5 3.3 1.4 0.2 setosa 7 3.2 4.7 1.4 versicolor 6.4 3.2 4.5 1.5 versicolor 6.9 3.1 4.9 1.5 versicolor 5.5 2.3 4 1.3 versicolor 6.5 2.8 4.6 1.5 versicolor 5.7 2.8 4.5 1.3 versicolor 6.3 3.3 4.7 1.6 versicolor 4.9 2.4 3.3 1 versicolor 6.6 2.9 4.6 1.3 versicolor 5.2 2.7 3.9 1.4 versicolor 5 2 3.5 1 versicolor 5.9 3 4.2 1.5 versicolor 6 2.2 4 1 versicolor 6.1 2.9 4.7 1.4 versicolor 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 5.6 3 4.5 1.5 versicolor 5.8 2.7 4.1 1 versicolor 6.2 2.2 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.1 2.8 4 1.3 versicolor 6.3 2.5 4.9 1.5 versicolor 6.1 2.8 4.7 1.2 versicolor 6.4 2.9 4.3 1.3 versicolor 6.6 3 4.4 1.4 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3 5 1.7 versicolor 6 2.9 4.5 1.5 versicolor 5.7 2.6 3.5 1 versicolor 5.5 2.4 3.8 1.1 versicolor 5.5 2.4 3.7 1 versicolor 5.8 2.7 3.9 1.2 versicolor 6 2.7 5.1 1.6 versicolor 5.4 3 4.5 1.5 versicolor 6 3.4 4.5 1.6 versicolor 6.7 3.1 4.7 1.5 versicolor 6.3 2.3 4.4 1.3 versicolor 5.6 3 4.1 1.3 versicolor 5.5 2.5 4 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 6.1 3 4.6 1.4 versicolor 5.8 2.6 4 1.2 versicolor 5 2.3 3.3 1 versicolor 5.6 2.7 4.2 1.3 versicolor 5.7 3 4.2 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.2 2.9 4.3 1.3 versicolor 5.1 2.5 3 1.1 versicolor 5.7 2.8 4.1 1.3 versicolor 6.3 3.3 6 2.5 virginica 5.8 2.7 5.1 1.9 virginica 7.1 3 5.9 2.1 virginica 6.3 2.9 5.6 1.8 virginica 6.5 3 5.8 2.2 virginica 7.6 3 6.6 2.1 virginica 4.9 2.5 4.5 1.7 virginica 7.3 2.9 6.3 1.8 virginica 6.7 2.5 5.8 1.8 virginica 7.2 3.6 6.1 2.5 virginica 6.5 3.2 5.1 2 virginica 6.4 2.7 5.3 1.9 virginica 6.8 3 5.5 2.1 virginica 5.7 2.5 5 2 virginica 5.8 2.8 5.1 2.4 virginica 6.4 3.2 5.3 2.3 virginica 6.5 3 5.5 1.8 virginica 7.7 3.8 6.7 2.2 virginica 7.7 2.6 6.9 2.3 virginica 6 2.2 5 1.5 virginica 6.9 3.2 5.7 2.3 virginica 5.6 2.8 4.9 2 virginica 7.7 2.8 6.7 2 virginica 6.3 2.7 4.9 1.8 virginica 6.7 3.3 5.7 2.1 virginica 7.2 3.2 6 1.8 virginica 6.2 2.8 4.8 1.8 virginica 6.1 3 4.9 1.8 virginica 6.4 2.8 5.6 2.1 virginica 7.2 3 5.8 1.6 virginica 7.4 2.8 6.1 1.9 virginica 7.9 3.8 6.4 2 virginica 6.4 2.8 5.6 2.2 virginica 6.3 2.8 5.1 1.5 virginica 6.1 2.6 5.6 1.4 virginica 7.7 3 6.1 2.3 virginica 6.3 3.4 5.6 2.4 virginica 6.4 3.1 5.5 1.8 virginica 6 3 4.8 1.8 virginica 6.9 3.1 5.4 2.1 virginica 6.7 3.1 5.6 2.4 virginica 6.9 3.1 5.1 2.3 virginica 5.8 2.7 5.1 1.9 virginica 6.8 3.2 5.9 2.3 virginica 6.7 3.3 5.7 2.5 virginica 6.7 3 5.2 2.3 virginica 6.3 2.5 5 1.9 virginica 6.5 3 5.2 2 virginica 6.2 3.4 5.4 2.3 virginica 5.9 3 5.1 1.8 virginica dataset . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 dataset . groupby ( 'class' ) . size () class setosa 50 versicolor 50 virginica 50 dtype: int64 dataset . plot ( kind = 'box' , subplots = True , layout = ( 2 , 4 ), figsize = ( 20 , 10 ), sharex = False , sharey = False ) plt . show () # histograms dataset . hist ( figsize = ( 20 , 10 )) plt . show () scatter_matrix ( dataset , figsize = ( 20 , 10 )) plt . show () # Split-out validation dataset array = dataset . values X = array [:, 0 : 4 ] Y = array [:, 4 ] # Make predictions on validation dataset seed = 1 def do_training ( train_size , classifierC ): X_train , X_validation , Y_train , Y_validation = \\ model_selection . train_test_split ( X , Y , \\ train_size = train_size , random_state = seed ) classifier = classifierC () classifier . fit ( X_train , Y_train ) p = classifier . predict ( X_validation ) return accuracy_score ( Y_validation , p ) trainset = [[ s / 10 , do_training ( s / 10 , LinearDiscriminantAnalysis ), do_training ( s / 10 , KNeighborsClassifier ), do_training ( s / 10 , DecisionTreeClassifier ), do_training ( s / 10 , GaussianNB ), ] for s in range ( 1 , 10 , 1 )] print ( \"Training Size & Classifier comparison at seed =\" , seed ) trainframe = DataFrame ( trainset , columns = [ \"Train_size (%)\" , \"LinearDiscriminantAnalysis\" , \"KNeighborsClassifier\" , \"DecisionTreeClassifier\" , \"GaussianNB\" ]) trainframe . plot ( x = 'Train_size (%)' , figsize = ( 20 , 10 )) table ( trainframe ) Training Size & Classifier comparison at seed = 1 Train_size (%) LinearDiscriminantAnalysis KNeighborsClassifier DecisionTreeClassifier GaussianNB 0.1 0.933333 0.948148 0.918519 0.948148 0.2 0.966667 0.933333 0.908333 0.933333 0.3 0.980952 0.980952 0.933333 0.961905 0.4 0.977778 0.977778 0.922222 0.966667 0.5 0.986667 0.946667 0.973333 0.96 0.6 0.983333 0.983333 0.966667 0.95 0.7 1 0.977778 0.955556 0.933333 0.8 1 1 0.966667 0.966667 0.9 1 1 1 1","title":"Iris Training"},{"location":"DPWD/DWPD_Iris/#iris-training","text":"Iris flower data set, Input: Output: from sklearn import datasets from pandas import * import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix from numpy import array from sklearn import model_selection from sklearn.metrics import classification_report , confusion_matrix , accuracy_score from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from IPython.display import HTML , display from tabulate import tabulate def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) # IRIS iris = datasets . load_iris () print ( iris . DESCR ) .. _iris_dataset : Iris plants dataset -------------------- ** Data Set Characteristics : ** : Number of Instances : 150 ( 50 in each of three classes ) : Number of Attributes : 4 numeric , predictive attributes and the class : Attribute Information : - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class : - Iris - Setosa - Iris - Versicolour - Iris - Virginica : Summary Statistics : ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length : 4 . 3 7 . 9 5 . 84 0 . 83 0 . 7826 sepal width : 2 . 0 4 . 4 3 . 05 0 . 43 - 0 . 4194 petal length : 1 . 0 6 . 9 3 . 76 1 . 76 0 . 9490 ( high ! ) petal width : 0 . 1 2 . 5 1 . 20 0 . 76 0 . 9565 ( high ! ) ============== ==== ==== ======= ===== ==================== : Missing Attribute Values : None : Class Distribution : 33 . 3 % for each of 3 classes . : Creator : R . A . Fisher : Donor : Michael Marshall ( MARSHALL % PLU @ io . arc . nasa . gov ) : Date : July , 1988 The famous Iris database , first used by Sir R . A . Fisher . The dataset is taken from Fisher ' s paper. Note that it ' s the same as in R , but not as in the UCI Machine Learning Repository , which has two wrong data points . This is perhaps the best known database to be found in the pattern recognition literature . Fisher ' s paper is a classic in the field and is referenced frequently to this day . ( See Duda & Hart , for example . ) The data set contains 3 classes of 50 instances each , where each class refers to a type of iris plant . One class is linearly separable from the other 2 ; the latter are NOT linearly separable from each other . .. topic :: References - Fisher , R . A . \" The use of multiple measurements in taxonomic problems \" Annual Eugenics , 7 , Part II , 179 - 188 ( 1936 ) ; also in \"Contributions to Mathematical Statistics \" (John Wiley, NY, 1950). - Duda , R . O ., & Hart , P . E . ( 1973 ) Pattern Classification and Scene Analysis . ( Q327 . D83 ) John Wiley & Sons . ISBN 0 - 471 - 22361 - 1 . See page 218 . - Dasarathy , B . V . ( 1980 ) \" Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments \" . IEEE Transactions on Pattern Analysis and Machine Intelligence , Vol . PAMI - 2 , No . 1 , 67 - 71 . - Gates , G . W . ( 1972 ) \" The Reduced Nearest Neighbor Rule \" . IEEE Transactions on Information Theory , May 1972 , 431 - 433 . - See also : 1988 MLC Proceedings , 54 - 64 . Cheeseman et al \" s AUTOCLASS II conceptual clustering system finds 3 classes in the data . - Many , many more ... # IRIS TRAINING OUTPUT data = [ list ( s ) + [ iris . target_names [ iris . target [ i ]]] for i , s in enumerate ( iris . data )] dataset = DataFrame ( data , columns = iris . feature_names + [ 'class' ]) table ( dataset ) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.1 3.5 1.4 0.2 setosa 4.9 3 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3 1.4 0.1 setosa 4.3 3 1.1 0.1 setosa 5.8 4 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa 5.4 3.4 1.7 0.2 setosa 5.1 3.7 1.5 0.4 setosa 4.6 3.6 1 0.2 setosa 5.1 3.3 1.7 0.5 setosa 4.8 3.4 1.9 0.2 setosa 5 3 1.6 0.2 setosa 5 3.4 1.6 0.4 setosa 5.2 3.5 1.5 0.2 setosa 5.2 3.4 1.4 0.2 setosa 4.7 3.2 1.6 0.2 setosa 4.8 3.1 1.6 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 5.5 4.2 1.4 0.2 setosa 4.9 3.1 1.5 0.2 setosa 5 3.2 1.2 0.2 setosa 5.5 3.5 1.3 0.2 setosa 4.9 3.6 1.4 0.1 setosa 4.4 3 1.3 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5 3.5 1.3 0.3 setosa 4.5 2.3 1.3 0.3 setosa 4.4 3.2 1.3 0.2 setosa 5 3.5 1.6 0.6 setosa 5.1 3.8 1.9 0.4 setosa 4.8 3 1.4 0.3 setosa 5.1 3.8 1.6 0.2 setosa 4.6 3.2 1.4 0.2 setosa 5.3 3.7 1.5 0.2 setosa 5 3.3 1.4 0.2 setosa 7 3.2 4.7 1.4 versicolor 6.4 3.2 4.5 1.5 versicolor 6.9 3.1 4.9 1.5 versicolor 5.5 2.3 4 1.3 versicolor 6.5 2.8 4.6 1.5 versicolor 5.7 2.8 4.5 1.3 versicolor 6.3 3.3 4.7 1.6 versicolor 4.9 2.4 3.3 1 versicolor 6.6 2.9 4.6 1.3 versicolor 5.2 2.7 3.9 1.4 versicolor 5 2 3.5 1 versicolor 5.9 3 4.2 1.5 versicolor 6 2.2 4 1 versicolor 6.1 2.9 4.7 1.4 versicolor 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 5.6 3 4.5 1.5 versicolor 5.8 2.7 4.1 1 versicolor 6.2 2.2 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.1 2.8 4 1.3 versicolor 6.3 2.5 4.9 1.5 versicolor 6.1 2.8 4.7 1.2 versicolor 6.4 2.9 4.3 1.3 versicolor 6.6 3 4.4 1.4 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3 5 1.7 versicolor 6 2.9 4.5 1.5 versicolor 5.7 2.6 3.5 1 versicolor 5.5 2.4 3.8 1.1 versicolor 5.5 2.4 3.7 1 versicolor 5.8 2.7 3.9 1.2 versicolor 6 2.7 5.1 1.6 versicolor 5.4 3 4.5 1.5 versicolor 6 3.4 4.5 1.6 versicolor 6.7 3.1 4.7 1.5 versicolor 6.3 2.3 4.4 1.3 versicolor 5.6 3 4.1 1.3 versicolor 5.5 2.5 4 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 6.1 3 4.6 1.4 versicolor 5.8 2.6 4 1.2 versicolor 5 2.3 3.3 1 versicolor 5.6 2.7 4.2 1.3 versicolor 5.7 3 4.2 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.2 2.9 4.3 1.3 versicolor 5.1 2.5 3 1.1 versicolor 5.7 2.8 4.1 1.3 versicolor 6.3 3.3 6 2.5 virginica 5.8 2.7 5.1 1.9 virginica 7.1 3 5.9 2.1 virginica 6.3 2.9 5.6 1.8 virginica 6.5 3 5.8 2.2 virginica 7.6 3 6.6 2.1 virginica 4.9 2.5 4.5 1.7 virginica 7.3 2.9 6.3 1.8 virginica 6.7 2.5 5.8 1.8 virginica 7.2 3.6 6.1 2.5 virginica 6.5 3.2 5.1 2 virginica 6.4 2.7 5.3 1.9 virginica 6.8 3 5.5 2.1 virginica 5.7 2.5 5 2 virginica 5.8 2.8 5.1 2.4 virginica 6.4 3.2 5.3 2.3 virginica 6.5 3 5.5 1.8 virginica 7.7 3.8 6.7 2.2 virginica 7.7 2.6 6.9 2.3 virginica 6 2.2 5 1.5 virginica 6.9 3.2 5.7 2.3 virginica 5.6 2.8 4.9 2 virginica 7.7 2.8 6.7 2 virginica 6.3 2.7 4.9 1.8 virginica 6.7 3.3 5.7 2.1 virginica 7.2 3.2 6 1.8 virginica 6.2 2.8 4.8 1.8 virginica 6.1 3 4.9 1.8 virginica 6.4 2.8 5.6 2.1 virginica 7.2 3 5.8 1.6 virginica 7.4 2.8 6.1 1.9 virginica 7.9 3.8 6.4 2 virginica 6.4 2.8 5.6 2.2 virginica 6.3 2.8 5.1 1.5 virginica 6.1 2.6 5.6 1.4 virginica 7.7 3 6.1 2.3 virginica 6.3 3.4 5.6 2.4 virginica 6.4 3.1 5.5 1.8 virginica 6 3 4.8 1.8 virginica 6.9 3.1 5.4 2.1 virginica 6.7 3.1 5.6 2.4 virginica 6.9 3.1 5.1 2.3 virginica 5.8 2.7 5.1 1.9 virginica 6.8 3.2 5.9 2.3 virginica 6.7 3.3 5.7 2.5 virginica 6.7 3 5.2 2.3 virginica 6.3 2.5 5 1.9 virginica 6.5 3 5.2 2 virginica 6.2 3.4 5.4 2.3 virginica 5.9 3 5.1 1.8 virginica dataset . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 dataset . groupby ( 'class' ) . size () class setosa 50 versicolor 50 virginica 50 dtype: int64 dataset . plot ( kind = 'box' , subplots = True , layout = ( 2 , 4 ), figsize = ( 20 , 10 ), sharex = False , sharey = False ) plt . show () # histograms dataset . hist ( figsize = ( 20 , 10 )) plt . show () scatter_matrix ( dataset , figsize = ( 20 , 10 )) plt . show () # Split-out validation dataset array = dataset . values X = array [:, 0 : 4 ] Y = array [:, 4 ] # Make predictions on validation dataset seed = 1 def do_training ( train_size , classifierC ): X_train , X_validation , Y_train , Y_validation = \\ model_selection . train_test_split ( X , Y , \\ train_size = train_size , random_state = seed ) classifier = classifierC () classifier . fit ( X_train , Y_train ) p = classifier . predict ( X_validation ) return accuracy_score ( Y_validation , p ) trainset = [[ s / 10 , do_training ( s / 10 , LinearDiscriminantAnalysis ), do_training ( s / 10 , KNeighborsClassifier ), do_training ( s / 10 , DecisionTreeClassifier ), do_training ( s / 10 , GaussianNB ), ] for s in range ( 1 , 10 , 1 )] print ( \"Training Size & Classifier comparison at seed =\" , seed ) trainframe = DataFrame ( trainset , columns = [ \"Train_size (%)\" , \"LinearDiscriminantAnalysis\" , \"KNeighborsClassifier\" , \"DecisionTreeClassifier\" , \"GaussianNB\" ]) trainframe . plot ( x = 'Train_size (%)' , figsize = ( 20 , 10 )) table ( trainframe ) Training Size & Classifier comparison at seed = 1 Train_size (%) LinearDiscriminantAnalysis KNeighborsClassifier DecisionTreeClassifier GaussianNB 0.1 0.933333 0.948148 0.918519 0.948148 0.2 0.966667 0.933333 0.908333 0.933333 0.3 0.980952 0.980952 0.933333 0.961905 0.4 0.977778 0.977778 0.922222 0.966667 0.5 0.986667 0.946667 0.973333 0.96 0.6 0.983333 0.983333 0.966667 0.95 0.7 1 0.977778 0.955556 0.933333 0.8 1 1 0.966667 0.966667 0.9 1 1 1 1","title":"Iris Training"}]}